<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>机器学习与数据挖掘期末复习 | XBXyftx</title><meta name="author" content="XBXyftx"><meta name="copyright" content="XBXyftx"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="计算机网络期末复习">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习与数据挖掘期末复习">
<meta property="og:url" content="https://xbxyftx.top/2026/01/09/MachineCollectionFinalReview/index.html">
<meta property="og:site_name" content="XBXyftx">
<meta property="og:description" content="计算机网络期末复习">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://xbxyftx.top/imgs/ArticleTopImgs/MachineCollectionFinalReviewTopImg.png">
<meta property="article:published_time" content="2026-01-09T06:46:13.000Z">
<meta property="article:modified_time" content="2026-01-11T08:47:48.366Z">
<meta property="article:author" content="XBXyftx">
<meta property="article:tag" content="期末复习">
<meta property="article:tag" content="机器学习与数据挖掘">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://xbxyftx.top/imgs/ArticleTopImgs/MachineCollectionFinalReviewTopImg.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "机器学习与数据挖掘期末复习",
  "url": "https://xbxyftx.top/2026/01/09/MachineCollectionFinalReview/",
  "image": "https://xbxyftx.top/imgs/ArticleTopImgs/MachineCollectionFinalReviewTopImg.png",
  "datePublished": "2026-01-09T06:46:13.000Z",
  "dateModified": "2026-01-11T08:47:48.366Z",
  "author": [
    {
      "@type": "Person",
      "name": "XBXyftx",
      "url": "https://github.com/XBXyftx"
    }
  ]
}</script><link rel="shortcut icon" href="/img/logo.png"><link rel="canonical" href="https://xbxyftx.top/2026/01/09/MachineCollectionFinalReview/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css"><link rel="stylesheet" href="/css/typewriter-effect.css"><link rel="stylesheet" href="/css/entrance-popup.css"><link rel="stylesheet" href="/css/lazy-loading.css"><link rel="stylesheet" href="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/node-snackbar/0.1.16/snackbar.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400,"highlightFullpage":true,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":2000,"languages":{"author":"作者: XBXyftx","link":"链接: ","source":"来源: XBXyftx","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'null',
  Snackbar: {"chs_to_cht":"已切换为繁体中文","cht_to_chs":"已切换为简体中文","day_to_night":"已切换为深色模式","night_to_day":"已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  infinitegrid: {
    js: '/',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: true,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '机器学习与数据挖掘期末复习',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post',
  typewriter: '📊 操作系统期末复习，一些真题的详解。'
}</script><script>localStorage.setItem('theme','dark');document.documentElement.setAttribute('data-theme','dark');</script><link rel="stylesheet" href="/css/universe.css"><link rel="stylesheet" href="/css/transpancy.css"><link rel="stylesheet" href="/css/styles.css"><link rel="stylesheet" href="/css/rightmenu.css"><link rel="stylesheet" href="/css/twikoo.css"><link rel="stylesheet" href="/css/lazy-loading-optimized.css"><link rel="stylesheet" href="/css/readmode-enhanced.css"><script src="/js/header-universe.js"></script><!-- 新增缓存管理脚本 - 已禁用Service Worker--><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="cat__scene"><div class="cat__main"><div class="cat__body"></div><div class="cat__body"></div><div class="cat__tail"></div><div class="cat__head"></div></div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
      setTimeout(() => {
        $loadingBox.style.display = 'none'
      }, 800)
    },
    initLoading: () => {
      $loadingBox.style.display = 'flex'
      $loadingBox.classList.remove('loaded')
      $body.style.overflow = 'hidden'
    }
  }

  preloader.initLoading()
  
  // 设置10秒超时
  const loadingTimeout = setTimeout(() => {
    preloader.endLoading()
  }, 10000)
  
  window.addEventListener('load', () => {
    clearTimeout(loadingTimeout)
    preloader.endLoading()
  })

  if (false) {
    btf.addGlobalFn('pjaxSend', preloader.initLoading, 'preloader_init')
    btf.addGlobalFn('pjaxComplete', preloader.endLoading, 'preloader_end')
  }
})()</script><div id="web_bg" style="background-image: url(https://bu.dusays.com/2025/07/04/6867890f5d403.png);"></div><div class="entrance-popup" id="entrance-popup"><div class="popup-content"><div class="popup-header"><span class="popup-title">温馨提示</span><span class="popup-close">×</span></div><div class="popup-body"><div class="popup-text"></div></div></div><div class="popup-overlay"></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-close-btn"><i class="fas fa-times"></i></div><div class="avatar-img text-center"><img src="/img/logo.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">45</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">44</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 文库</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/coffer/"><i class="fa-fw fa-solid fa-key"></i><span> 秘密基地</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa-solid fa-i"></i><span> Myself</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></li><li><a class="site-page child" href="/swiper/"><i class="fa-fw fa-solid fa-star-of-david"></i><span> 昨日重现</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fa-solid fa-comment"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/LianlianKan/"><i class="fa-fw fa-solid fa-gamepad"></i><span> 连连看</span></a></div><div class="menus_item"><a class="site-page" href="/MarkdownPreview/"><i class="fa-fw fa-solid fa-file-code"></i><span> MD在线编辑器</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/imgs/ArticleTopImgs/MachineCollectionFinalReviewTopImg.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/img/logo.png" alt="Logo"><span class="site-name">XBXyftx</span></a><a class="nav-page-title" href="/"><span class="site-name">机器学习与数据挖掘期末复习</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 文库</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/coffer/"><i class="fa-fw fa-solid fa-key"></i><span> 秘密基地</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa-solid fa-i"></i><span> Myself</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></li><li><a class="site-page child" href="/swiper/"><i class="fa-fw fa-solid fa-star-of-david"></i><span> 昨日重现</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fa-solid fa-comment"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/LianlianKan/"><i class="fa-fw fa-solid fa-gamepad"></i><span> 连连看</span></a></div><div class="menus_item"><a class="site-page" href="/MarkdownPreview/"><i class="fa-fw fa-solid fa-file-code"></i><span> MD在线编辑器</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">机器学习与数据挖掘期末复习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2026-01-09T06:46:13.000Z" title="发表于 2026-01-09 14:46:13">2026-01-09</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-01-11T08:47:48.366Z" title="更新于 2026-01-11 16:47:48">2026-01-11</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">20.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>72分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><div id="post-outdate-notice" data="{&quot;limitDay&quot;:100,&quot;messagePrev&quot;:&quot;It has been&quot;,&quot;messageNext&quot;:&quot;days since the last update, the content of the article may be outdated.&quot;,&quot;postUpdate&quot;:&quot;2026-01-11 16:47:48&quot;}" hidden></div><h2 id="作业题汇总"><a href="#作业题汇总" class="headerlink" title="作业题汇总"></a>作业题汇总</h2><h3 id="作业1"><a href="#作业1" class="headerlink" title="作业1"></a>作业1</h3><h4 id="连续属性与离散属性"><a href="#连续属性与离散属性" class="headerlink" title="连续属性与离散属性"></a>连续属性与离散属性</h4><p><img src="/2026/01/09/MachineCollectionFinalReview/1.png" alt="1"></p>
<ol>
<li>正确选项：A、C</li>
<li>解析<ul>
<li>连续属性的核心特点：取值可覆盖某一区间内的任意数值，能无限细分。</li>
<li>选项判断：<ul>
<li>A. 人的身高：可取170cm、170.2cm等任意精度的数值，符合连续属性定义。</li>
<li>B. 人的性别：仅能取“男”“女”等固定类别，属于离散属性。</li>
<li>C. 人的体重：可取50kg、50.1kg等任意精度的数值，符合连续属性定义。</li>
<li>D. 人的国籍：仅能取有限的国家名称，属于离散属性。</li>
</ul>
</li>
</ul>
</li>
<li>离散属性与连续属性对比</li>
</ol>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性类型</th>
<th>特点</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td>离散属性</td>
<td>取值为有限类别/离散值，不可细分</td>
<td>性别、国籍、学历</td>
</tr>
<tr>
<td>连续属性</td>
<td>取值为区间内任意数值，可无限细分</td>
<td>身高、体重、温度</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h4 id="离群点（Outliers）"><a href="#离群点（Outliers）" class="headerlink" title="离群点（Outliers）"></a>离群点（Outliers）</h4><p><img src="/2026/01/09/MachineCollectionFinalReview/2.png" alt="2"></p>
<ol>
<li>正确选项：B、C</li>
<li>解析调整<ul>
<li>对选项C的补充说明：离群点本身是<strong>合法数据</strong>（并非错误），其是否保留需结合场景——若分析场景需要覆盖“特殊样本”（比如研究某群体的极端情况），离群点应保留；仅当分析目标是“多数样本的共性”时，才考虑剔除。因此“是合法的数据对象，数据预处理中要保留”这一表述，在“需保留离群点的场景”下是成立的。</li>
</ul>
</li>
<li>修正后的选项分析<ul>
<li>A. 错误：离群点是合法但特征特殊的数据，噪声是数据错误，二者本质不同。</li>
<li>B. 正确：符合离群点的核心定义（与大部分数据特征差异显著）。</li>
<li>C. 正确：离群点是合法数据，在需要分析特殊样本的场景中，预处理时应保留。</li>
<li>D. 错误：离群点是合法数据，并非必须滤除。</li>
</ul>
</li>
</ol>
<hr>
<h4 id="数据挖掘的定义"><a href="#数据挖掘的定义" class="headerlink" title="数据挖掘的定义"></a>数据挖掘的定义</h4><p><img src="/2026/01/09/MachineCollectionFinalReview/3.png" alt="3"></p>
<ol>
<li><p><strong>正确选项：C、D、E</strong></p>
</li>
<li><p><strong>解析</strong></p>
<ul>
<li><p>数据挖掘的核心目标：<br>从大量数据中<strong>自动发现潜在的模式、规律或知识</strong>，而不仅仅是执行简单的查找或匹配操作。</p>
</li>
<li><p>判断关键标准：<br>是否涉及<strong>模式发现、预测、分类、异常检测等智能分析过程</strong>。</p>
</li>
<li><p>选项判断：</p>
<ul>
<li>A. 信用卡欺诈消费检测：<br>通过分析大量历史交易数据，识别异常消费行为，属于异常检测与分类问题，是典型的数据挖掘应用。</li>
<li>B. 网络入侵检测：<br>通过分析网络流量和行为模式，发现潜在攻击或异常行为，属于模式识别与分类，属于数据挖掘应用。</li>
<li>C. 共享文档中搜索一个关键字：<br>属于基于关键词的字符串匹配或信息检索，不涉及模式发现，不属于数据挖掘。</li>
<li>D. 京东上搜索一本书：<br>搜索行为本质是信息检索（关键词匹配），不等同于基于用户行为的推荐系统，因此不属于数据挖掘。</li>
<li>E. 手机通讯录上通过电话找一个联系人：<br>属于简单查询操作，仅进行精确匹配，不涉及数据分析或知识发现。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>数据挖掘与信息检索对比</strong></p>
</li>
</ol>
<div class="table-container">
<table>
<thead>
<tr>
<th>对比维度</th>
<th>数据挖掘</th>
<th>信息检索 / 查询</th>
</tr>
</thead>
<tbody>
<tr>
<td>核心目的</td>
<td>发现潜在规律和知识</td>
<td>查找已知目标</td>
</tr>
<tr>
<td>是否产生新知识</td>
<td>是</td>
<td>否</td>
</tr>
<tr>
<td>是否需要模型或算法</td>
<td>通常需要</td>
<td>通常不需要</td>
</tr>
<tr>
<td>自动化与智能性</td>
<td>高</td>
<td>低</td>
</tr>
<tr>
<td>典型应用</td>
<td>欺诈检测、入侵检测、用户画像</td>
<td>文档搜索、联系人查找</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h4 id="均值与中值"><a href="#均值与中值" class="headerlink" title="均值与中值"></a>均值与中值</h4><p><img src="/2026/01/09/MachineCollectionFinalReview/4.png" alt="4"></p>
<ol>
<li>均值计算：<br>均值 = （1 + 3 + 5 + 7 + 9 + 95）÷ 6 = 120 ÷ 6 = 20</li>
<li>中值计算：<br>先将数据集按升序排列：1、3、5、7、9、95<br>数据个数为偶数时，中值是中间两个数的平均值，即（5 + 7）÷ 2 = 6</li>
<li>答案：均值是20，中值是6</li>
</ol>
<hr>
<h4 id="混淆矩阵：精度与召回率"><a href="#混淆矩阵：精度与召回率" class="headerlink" title="混淆矩阵：精度与召回率"></a>混淆矩阵：精度与召回率</h4><p><img src="/2026/01/09/MachineCollectionFinalReview/5.png" alt="5"></p>
<ol>
<li><p>先明确混淆矩阵的四个核心指标：</p>
<ul>
<li>真正例（TP）：实际患病且被诊断为患病的人数 = 48</li>
<li>假正例（FP）：实际健康但被诊断为患病的人数 = 2</li>
<li>假反例（FN）：实际患病但被诊断为健康的人数 = 60 - 48 = 12</li>
<li>真反例（TN）：实际健康且被诊断为健康的人数 = 40 - 2 = 38</li>
</ul>
</li>
<li><p>精度（Precision）的计算：<br>精度 = TP / (TP + FP) = 48 / (48 + 2) = 48 / 50 = 0.96（或96%）</p>
</li>
<li><p>召回率（Recall）的计算：<br>召回率 = TP / (TP + FN) = 48 / (48 + 12) = 48 / 60 = 0.8（或80%）</p>
</li>
<li><p>答案：精度是0.96，召回率是0.8</p>
</li>
</ol>
<hr>
<h3 id="作业2"><a href="#作业2" class="headerlink" title="作业2"></a>作业2</h3><h4 id="离散属性编码"><a href="#离散属性编码" class="headerlink" title="离散属性编码"></a>离散属性编码</h4><p><img src="/2026/01/09/MachineCollectionFinalReview/6.png" alt="6"></p>
<ul>
<li><p>最少二元属性数量计算<br>  要表示6种取值，需满足  2^k ≥ 6，解得 k = 3（因为 2^2 = 4 &lt; 6，2^3 = 8 ≥ 6），因此最少需要3个二元属性。</p>
</li>
<li><p>两种编码方案<br>  方案1：最少二元属性编码（二进制编码）<br>  用3个二元属性（记为A、B、C），将6种尺码映射为3位二进制数：</p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>尺码</th>
<th>二元属性A</th>
<th>二元属性B</th>
<th>二元属性C</th>
</tr>
</thead>
<tbody>
<tr>
<td>S</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>M</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>L</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>XL</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>XXL</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>XXXL</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>方案2：One-Hot（独热）编码<br>需与取值数量相等的二元属性（6个），每个属性对应一种尺码，取值为1表示对应尺码，0表示不对应：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>尺码</th>
<th>属性1（S）</th>
<th>属性2（M）</th>
<th>属性3（L）</th>
<th>属性4（XL）</th>
<th>属性5（XXL）</th>
<th>属性6（XXXL）</th>
</tr>
</thead>
<tbody>
<tr>
<td>S</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>M</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>L</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>XL</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>XXL</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>XXXL</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>结论<ul>
<li>最少二元属性数量：3个</li>
<li>One-Hot编码所需二元属性数量：6个</li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>编码类型</th>
<th>核心原理</th>
<th>适用场景</th>
<th>优点</th>
<th>缺点</th>
<th>示例（以属性“尺码：S、M、L、XL、XXL”为例）</th>
</tr>
</thead>
<tbody>
<tr>
<td>二进制编码</td>
<td>将离散属性的每个取值映射为整数，再转换为二进制数，用二进制位对应二元属性</td>
<td>离散属性取值较少、无明确序关系</td>
<td>占用特征维度少（仅需$\lceil log_2m \rceil$个）</td>
<td>引入虚假序关系，扭曲属性间距离</td>
<td>整数映射：S=0、M=1、L=2、XL=3、XXL=4；二进制属性：X1（高位）、X2、X3（低位）<br>S(0)→000，M(1)→001，L(2)→010，XL(3)→011，XXL(4)→100</td>
</tr>
<tr>
<td>独热编码（One-Hot）</td>
<td>为每个属性取值创建独立二元属性，取值为1表示对应属性值，0表示不对应</td>
<td>离散属性无序号关系（如性别、国籍）</td>
<td>无虚假序关系，保留属性原始分布</td>
<td>特征维度爆炸（需m个二元属性）</td>
<td>属性1（S）、属性2（M）、属性3（L）、属性4（XL）、属性5（XXL）<br>S→10000，M→01000，L→00100，XL→00010，XXL→00001</td>
</tr>
<tr>
<td>标签编码（Label Encoding）</td>
<td>直接将离散属性的每个取值映射为唯一整数</td>
<td>有序离散属性（如等级：差、中、好）</td>
<td>编码简单，占用维度最少（仅1个）</td>
<td>非有序属性会引入虚假序关系</td>
<td>S→0，M→1，L→2，XL→3，XXL→4</td>
</tr>
<tr>
<td>目标编码（Target Encoding）</td>
<td>用属性取值对应的目标变量统计值（如均值、中位数）作为编码值</td>
<td>高基数离散属性（如城市、用户ID）</td>
<td>不增加维度，保留目标相关信息</td>
<td>易过拟合，需交叉验证避免泄露</td>
<td>若目标为“是否购买”，S对应购买率30%→编码0.3，M对应购买率50%→编码0.5，以此类推</td>
</tr>
<tr>
<td>计数编码（Count Encoding）</td>
<td>用属性取值在数据集中的出现次数作为编码值</td>
<td>高基数离散属性，无明显目标关联</td>
<td>编码简单，不增加维度</td>
<td>相同计数的取值无法区分，可能丢失信息</td>
<td>若S出现20次→20，M出现35次→35，L出现30次→30，XL出现15次→15，XXL出现10次→10</td>
</tr>
</tbody>
</table>
</div>
<hr>
<p><img src="/2026/01/09/MachineCollectionFinalReview/7.png" alt="7"></p>
<p><img src="/2026/01/09/MachineCollectionFinalReview/8.png" alt="8"></p>
<p><img src="/2026/01/09/MachineCollectionFinalReview/9.png" alt="9"></p>
<p><img src="/2026/01/09/MachineCollectionFinalReview/10.png" alt="10"></p>
<p><img src="/2026/01/09/MachineCollectionFinalReview/11.png" alt="11"></p>
<p><img src="/2026/01/09/MachineCollectionFinalReview/12.png" alt="12"></p>
<p><img src="/2026/01/09/MachineCollectionFinalReview/13.png" alt="13"></p>
<p><img src="/2026/01/09/MachineCollectionFinalReview/14.png" alt="14"></p>
<p><img src="/2026/01/09/MachineCollectionFinalReview/15.png" alt="15"></p>
<p><img src="/2026/01/09/MachineCollectionFinalReview/16.png" alt="16"></p>
<hr>
<h4 id="PR曲线与ROC曲线"><a href="#PR曲线与ROC曲线" class="headerlink" title="PR曲线与ROC曲线"></a>PR曲线与ROC曲线</h4><p><img src="/2026/01/09/MachineCollectionFinalReview/17.png" alt="17"></p>
<p>先看一下pr曲线和ROC曲线的ppt</p>
<p><img src="/2026/01/09/MachineCollectionFinalReview/18.png" alt="18"></p>
<ol>
<li><p>描述对应的评价标准<br> 标准：</p>
<ul>
<li>(a) 对应<strong>查准率（精度）</strong>：“抓的人当中真正是小偷的比例”，即“预测为正例的样本中实际正例的比例”，符合查准率的定义。</li>
<li>(b) 对应<strong>查全率（召回率）</strong>：“所有小偷中被抓到的比例”，即“实际正例的样本中被预测为正例的比例”，符合召回率的定义。</li>
</ul>
</li>
<li><p>人脸检测算法的精度与召回率计算<br> 首先确定混淆矩阵指标：</p>
<ul>
<li>真正例（TP）：实际是人脸且被检测为人脸的数量 = 40</li>
<li>假正例（FP）：实际非人脸但被检测为人脸的数量 = 10</li>
<li>假反例（FN）：实际是人脸但未被检测为人脸的数量 = 80 - 40 = 40</li>
</ul>
</li>
</ol>
<ul>
<li><p><strong>精度（Precision）</strong>：<br><img src="/2026/01/09/MachineCollectionFinalReview/19.png" alt="19"></p>
</li>
<li><p><strong>召回率（Recall）</strong>：<br><img src="/2026/01/09/MachineCollectionFinalReview/20.png" alt="20"></p>
</li>
</ul>
<p>结果汇总</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>评价标准</th>
<th>数值</th>
</tr>
</thead>
<tbody>
<tr>
<td>精度</td>
<td>0.8</td>
</tr>
<tr>
<td>召回率</td>
<td>0.5</td>
</tr>
</tbody>
</table>
</div>
<p><img src="/2026/01/09/MachineCollectionFinalReview/21.png" alt="21"></p>
<p>但是ROC在复习ppt中没有所以我选择先放掉。</p>
<p>回到题目，人脸检测算法的精度与召回率计算。</p>
<p><img src="/2026/01/09/MachineCollectionFinalReview/17.png" alt="17"></p>
<p>首先解释一下这个表格，有点怪但也还可以懂。我们将每一步的预测结果都列出来就会好理解很多。</p>
<p>首先明确：要计算不同阈值下的精度（查准率）和召回率（查全率），需先按<strong>预测概率从高到低排序样本</strong>，再依次以每个样本的概率为阈值（将≥该阈值的样本判定为“包含人脸”），计算对应指标。</p>
<p>步骤1：排序样本（按预测概率降序）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>样本序号</th>
<th>真实标签（是否有人脸）</th>
<th>预测概率</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>是（正例）</td>
<td>0.9</td>
</tr>
<tr>
<td>2</td>
<td>是（正例）</td>
<td>0.7</td>
</tr>
<tr>
<td>3</td>
<td>否（反例）</td>
<td>0.5</td>
</tr>
<tr>
<td>4</td>
<td>否（反例）</td>
<td>0.3</td>
</tr>
<tr>
<td>5</td>
<td>是（正例）</td>
<td>0.1</td>
</tr>
</tbody>
</table>
</div>
<p>步骤2：逐阈值计算精度和召回率<br>总正例数（真实有人脸的样本数）=3，总反例数（真实无人脸的样本数）=2。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>阈值（判定为“有人脸”的最低概率）</th>
<th>判定为“有人脸”的样本</th>
<th>TP（真实有人脸且判定正确的数量）</th>
<th>FP（真实无人脸但判定错误的数量）</th>
<th>精度（Precision）= TP/(TP+FP)</th>
<th>召回率（Recall）= TP/总正例</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.9（仅样本1满足）</td>
<td>样本1</td>
<td>1</td>
<td>0</td>
<td>1/1=1</td>
<td>1/3≈0.33</td>
</tr>
<tr>
<td>0.7（样本1、2满足）</td>
<td>样本1、2</td>
<td>2</td>
<td>0</td>
<td>2/2=1</td>
<td>2/3≈0.67</td>
</tr>
<tr>
<td>0.5（样本1、2、3满足）</td>
<td>样本1、2、3</td>
<td>2</td>
<td>1</td>
<td>2/3≈0.67</td>
<td>2/3≈0.67</td>
</tr>
<tr>
<td>0.3（样本1、2、3、4满足）</td>
<td>样本1、2、3、4</td>
<td>2</td>
<td>2</td>
<td>2/4=0.5</td>
<td>2/3≈0.67</td>
</tr>
<tr>
<td>0.1（所有样本满足）</td>
<td>所有5个样本</td>
<td>3</td>
<td>2</td>
<td>3/5=0.6</td>
<td>3/3=1</td>
</tr>
</tbody>
</table>
</div>
<p>每个阈值的选取仅有大于等于这个阈值的样本会被判定为“有人脸”，剩余的都是“否”。所以接下来我们来手动算一遍这个过程。</p>
<p>但在此之前我们还是需要先去明确一下TFPN之间的关系和代表的含义。</p>
<p>在分类任务（比如人脸检测）中，TP、FP、TN、FN是混淆矩阵的4个核心指标，对应“真实标签”和“预测结果”的4种组合，用人脸检测的场景可以更直观理解：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>指标</th>
<th>英文全称</th>
<th>含义（以“人脸检测”为例）</th>
</tr>
</thead>
<tbody>
<tr>
<td>TP</td>
<td>True Positive</td>
<td>真实标签是“有人脸”，预测结果也判定为“有人脸”（判定正确的正例）</td>
</tr>
<tr>
<td>FP</td>
<td>False Positive</td>
<td>真实标签是“无人脸”，预测结果却判定为“有人脸”（判定错误的反例，误报）</td>
</tr>
<tr>
<td>TN</td>
<td>True Negative</td>
<td>真实标签是“无人脸”，预测结果也判定为“无人脸”（判定正确的反例）</td>
</tr>
<tr>
<td>FN</td>
<td>False Negative</td>
<td>真实标签是“有人脸”，预测结果却判定为“无人脸”（判定错误的正例，漏报）</td>
</tr>
</tbody>
</table>
</div>
<p>以题目中的样本为例（阈值=0.5时）：</p>
<ul>
<li>TP=2：2个真实有人脸的样本被正确判定为“有人脸”</li>
<li>FP=1：1个真实无人脸的样本被错误判定为“有人脸”</li>
<li>TN=1：1个真实无人脸的样本被正确判定为“无人脸”</li>
<li>FN=1：1个真实有人脸的样本被错误判定为“无人脸”</li>
</ul>
<div class="note success flat"><p>我自己总结的经验就是第一个字母（T/F）：表示 “预测结果是否正确”（T = 正确，F = 错误）</p>
<p>而第二个字母（P/N）：表示 “预测结果的类别”（P = 预测为正例，N = 预测为反例）</p>
<p>由此我们可以总结出：</p>
<ul>
<li>TP：样本本身是正例，同时预测正确（预测为正例）例</li>
<li>FP：样本本身是反例，但预测错误（预测为正例）</li>
<li>TN：样本本身是反例，同时预测正确（预测为反例）例</li>
<li>FN：样本本身是正例，但预测错误（预测为反例）</li>
</ul>
</div>
<p><img src="/2026/01/09/MachineCollectionFinalReview/22.png" alt="22"></p>
<p>随后，ROC曲线的话，大致是这样的：其X轴是<strong>假正例率（FPR）</strong>，Y轴是<strong>真正例率（TPR）</strong>。</p>
<p><img src="/2026/01/09/MachineCollectionFinalReview/23.png" alt="23"></p>
<hr>
<h3 id="作业3"><a href="#作业3" class="headerlink" title="作业3"></a>作业3</h3><h4 id="线性回归中的属性处理"><a href="#线性回归中的属性处理" class="headerlink" title="线性回归中的属性处理"></a>线性回归中的属性处理</h4><p><img src="/2026/01/09/MachineCollectionFinalReview/24.png" alt="24"></p>
<ul>
<li>答案：正确选项是AD</li>
<li>解析：<ul>
<li>对于作业提交次数：<ul>
<li>作业提交次数是数值型属性（取值0-10），可直接作为连续数值参与线性回归，因此A选项正确；</li>
<li>B选项错误，作业提交次数本身是数值，无需转化为向量（向量编码适用于分类属性）。</li>
</ul>
</li>
<li>对于民族：<ul>
<li>民族是分类属性（无顺序关系的类别），需用独热编码转化为向量：56个民族对应55维向量（排除基准类别），因此D选项正确；</li>
<li>C选项错误，将民族映射为0-55的数值会错误引入“类别有序”的假设，不符合线性回归的属性要求。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>可以通过表格对比这两种属性的特点及处理方式：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性类型</th>
<th>作业提交次数</th>
<th>民族</th>
</tr>
</thead>
<tbody>
<tr>
<td>属性性质</td>
<td>数值型（离散数值）</td>
<td>分类型（无序类别）</td>
</tr>
<tr>
<td>取值特点</td>
<td>0-10的连续数值（有大小意义）</td>
<td>56个无顺序的类别（无大小意义）</td>
</tr>
<tr>
<td>合适的处理方式</td>
<td>直接作为数值参与计算</td>
<td>独热编码转化为向量</td>
</tr>
<tr>
<td>对应选项正确性</td>
<td>A正确、B错误</td>
<td>D正确、C错误</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h4 id="线性回归与RANSAC算法"><a href="#线性回归与RANSAC算法" class="headerlink" title="线性回归与RANSAC算法"></a>线性回归与RANSAC算法</h4><p><img src="/2026/01/09/MachineCollectionFinalReview/25.png" alt="25"></p>
<ul>
<li>答案：正确选项是AF</li>
<li>解析：<ul>
<li>关于方法特点：<ul>
<li>一元线性回归会拟合所有数据（包括异常点），结果易受异常点影响，图中红线偏离多数数据点，符合一元线性回归的特点，因此A正确；</li>
<li>RANSAC是鲁棒回归方法，会拟合数据中的“内点”（多数正常数据），图中绿线贴合多数黑色星号数据，符合RANSAC的特点，因此F正确；</li>
</ul>
</li>
<li>排除错误选项：<ul>
<li>题目中只有一个输入属性（图为二维散点，对应一元回归），因此B、E中“二元线性回归”不符合场景；</li>
<li>C错误（红线不符合RANSAC拟合内点的特点），D错误（绿线不符合一元线性回归受异常点干扰的特点）。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="RANSAC-RANdom-SAmple-Consensus-算法"><a href="#RANSAC-RANdom-SAmple-Consensus-算法" class="headerlink" title="RANSAC (RANdom SAmple Consensus) 算法"></a>RANSAC (RANdom SAmple Consensus) 算法</h4><p>算法步骤：</p>
<ul>
<li>1）从原始数据中随机选择一些样本作为“内点”</li>
<li>2）用1中选择的样本拟合模型</li>
<li>3）利用模型计算其它样本的残差，若某个样本的残差小于预先设置的阈值t，则将其加入内点，将内点中的样本量扩充</li>
<li>4）用扩充后的内点拟合模型，计算均方误差</li>
<li>5）重复1-4步，最终选取均方误差最小的模型</li>
</ul>
<p>随机选出一个样本作为“内点”的步骤，可以理解为“随机采样”步骤，因此RANSAC算法也被称为“随机采样一致性”算法。</p>
<p>随后找的“内点”也就是离得距离近的点，距离近的点最多的模型就是最好的模型。是一个反复随机抽样枚举出最好模型的方法不是像一元线性回归那样直接拟合所有数据，所以其获得的模型一定是会天然的避免异常点离群点的影响的。</p>
<p><img src="/2026/01/09/MachineCollectionFinalReview/26.png" alt="26"></p>
<p>这张图展示了<strong>RANSAC（随机抽样一致）算法拟合直线的完整流程</strong>，分步骤解释如下：</p>
<ul>
<li><strong>Sample（抽样）</strong>：<ul>
<li>从数据中随机选取少量“候选内点”（图中绿色点），用这些点拟合一条初始直线。</li>
</ul>
</li>
<li><strong>Solve（求解）</strong>：<ul>
<li>基于选中的候选内点，计算出对应的直线模型（图中蓝色直线）。</li>
</ul>
</li>
<li><strong>Score（评分）</strong>：<ul>
<li>统计所有数据中，与当前直线模型误差在阈值内的点（即“内点”，图中黄色点）数量：<ul>
<li>步骤1：第一次拟合的直线仅得到6个内点；</li>
<li>步骤2：调整模型后得到11个内点；</li>
<li>步骤3：再次调整后内点数量减少到4个；</li>
<li>步骤4：最终得到14个内点（满足“内点数量＞最小共识内点”的条件）。</li>
</ul>
</li>
</ul>
</li>
<li><strong>最终步骤</strong>：<ul>
<li>当内点数量达到要求时，用所有内点重新优化直线模型，得到鲁棒的拟合结果。</li>
</ul>
</li>
</ul>
<p><img src="/2026/01/09/MachineCollectionFinalReview/27.png" alt="27"></p>
<p>这张图对比了<strong>普通线性回归与RANSAC回归的拟合效果</strong>，具体解释如下：</p>
<ul>
<li>元素说明：<ul>
<li>绿色点（Inliers）：数据中的“内点”（多数正常数据）；</li>
<li>黄色点（Outliers）：数据中的“异常点”（偏离整体趋势的噪声点）；</li>
<li>深蓝色线：普通线性回归的拟合结果；</li>
<li>浅蓝色线：RANSAC回归的拟合结果。</li>
</ul>
</li>
<li>效果对比：<ul>
<li>普通线性回归（深蓝色线）会被异常点（黄色点）“拉偏”，拟合结果偏离了多数内点的趋势；</li>
<li>RANSAC回归（浅蓝色线）只拟合内点（绿色点），不受异常点干扰，更贴合数据的真实趋势。</li>
</ul>
</li>
</ul>
<hr>
<p><img src="/2026/01/09/MachineCollectionFinalReview/28.png" alt="28"></p>
<ul>
<li>答案：正确选项是D</li>
</ul>
<hr>
<h4 id="正则化方法"><a href="#正则化方法" class="headerlink" title="正则化方法"></a>正则化方法</h4><p><img src="/2026/01/09/MachineCollectionFinalReview/29.png" alt="29"></p>
<ul>
<li>答案：<ol>
<li>当q=2时被称为<strong>ridge（岭</strong>回归</li>
<li>当q=1时被称为<strong>Lasso</strong>回归</li>
</ol>
</li>
<li>解析：<ul>
<li>正则项中q=2对应的是L2范数，基于L2范数的线性回归称为岭回归，它会让回归参数w的取值更平缓，避免参数过大导致过拟合；</li>
<li>正则项中q=1对应的是L1范数，基于L1范数的线性回归称为Lasso回归，它不仅能缓解过拟合，还能实现特征选择（让部分参数w变为0）。</li>
</ul>
</li>
</ul>
<p>以下是常见正则化方法的特点对比表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>正则化方法</th>
<th>对应范数</th>
<th>核心特点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>岭回归</td>
<td>L2范数$\lVert w \rVert_2$</td>
<td>让回归参数取值更平缓，避免参数过大；参数不会被压缩至0</td>
<td>特征维度高、存在多重共线性的场景</td>
</tr>
<tr>
<td>Lasso回归</td>
<td>L1范数$\lVert w \rVert_1$</td>
<td>缓解过拟合的同时，可实现特征选择（部分参数被压缩至0）</td>
<td>需要筛选关键特征的场景</td>
</tr>
<tr>
<td>Elastic Net（弹性网）</td>
<td>L1+L2范数</td>
<td>结合岭回归和Lasso的优点，既避免参数过大，也能进行特征选择</td>
<td>特征数量远多于样本量的场景</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h4 id="岭回归（Ridge-Regression）与-Lasso回归（Lasso-Regression）"><a href="#岭回归（Ridge-Regression）与-Lasso回归（Lasso-Regression）" class="headerlink" title="岭回归（Ridge Regression）与 Lasso回归（Lasso Regression）"></a>岭回归（Ridge Regression）与 Lasso回归（Lasso Regression）</h4><p><img src="/2026/01/09/MachineCollectionFinalReview/30.png" alt="30"></p>
<p><img src="/2026/01/09/MachineCollectionFinalReview/31.png" alt="31"></p>
<p><img src="/2026/01/09/MachineCollectionFinalReview/32.png" alt="32"></p>
<p><img src="/2026/01/09/MachineCollectionFinalReview/33.png" alt="33"></p>
<h5 id="核心概念对比"><a href="#核心概念对比" class="headerlink" title="核心概念对比"></a>核心概念对比</h5><p><strong>岭回归（Ridge Regression）</strong></p>
<ul>
<li><strong>正则化项</strong>：L2范数 $\lambda \sum_{j=1}^{p} w_j^2$</li>
<li><strong>几何解释</strong>：约束条件为圆形区域，参数向量被限制在圆内</li>
<li><strong>参数特点</strong>：参数会被”收缩”但不会变为0，所有特征都会保留</li>
<li><strong>适用场景</strong>：特征间存在多重共线性，需要稳定模型但保留所有特征</li>
</ul>
<p><strong>Lasso回归（Lasso Regression）</strong></p>
<ul>
<li><strong>正则化项</strong>：L1范数 $\lambda \sum_{j=1}^{p} |w_j|$</li>
<li><strong>几何解释</strong>：约束条件为菱形区域，容易在顶点处产生稀疏解</li>
<li><strong>参数特点</strong>：部分参数会被压缩至0，实现自动特征选择</li>
<li><strong>适用场景</strong>：特征维度高，需要筛选重要特征并简化模型</li>
</ul>
<h5 id="数学推导要点"><a href="#数学推导要点" class="headerlink" title="数学推导要点"></a>数学推导要点</h5><p><strong>岭回归的闭式解</strong>：</p>
<script type="math/tex; mode=display">\hat{w}_{ridge} = (X^TX + \lambda I)^{-1}X^Ty</script><p>其中$\lambda$是正则化参数，$I$是单位矩阵。</p>
<p><strong>关键特性</strong>：</p>
<ol>
<li><strong>可逆性保证</strong>：即使$X^TX$不可逆，$(X^TX + \lambda I)$也是可逆的</li>
<li><strong>参数收缩</strong>：$\lambda$越大，参数向量的模长越小</li>
<li><strong>偏差-方差权衡</strong>：增加偏差来减少方差，提高泛化能力</li>
</ol>
<p><strong>Lasso回归的优化</strong>：<br>由于L1范数不可导，通常使用：</p>
<ul>
<li><strong>坐标下降法</strong>：逐个优化参数</li>
<li><strong>软阈值算子</strong>：$S_{\lambda}(z) = \text{sign}(z) \max(|z| - \lambda, 0)$</li>
</ul>
<h5 id="正则化参数λ的选择"><a href="#正则化参数λ的选择" class="headerlink" title="正则化参数λ的选择"></a>正则化参数λ的选择</h5><p><strong>交叉验证法</strong>：</p>
<ol>
<li>将训练集分为K折</li>
<li>对不同的λ值，计算K折交叉验证误差</li>
<li>选择使验证误差最小的λ值</li>
</ol>
<p><strong>λ值效果</strong>：</p>
<ul>
<li>λ = 0：退化为普通线性回归，可能过拟合</li>
<li>λ过小：正则化效果不明显</li>
<li>λ适中：平衡拟合效果与模型复杂度</li>
<li>λ过大：欠拟合，模型过于简单</li>
</ul>
<h5 id="实际应用对比"><a href="#实际应用对比" class="headerlink" title="实际应用对比"></a>实际应用对比</h5><div class="table-container">
<table>
<thead>
<tr>
<th>对比维度</th>
<th>岭回归</th>
<th>Lasso回归</th>
</tr>
</thead>
<tbody>
<tr>
<td>特征选择</td>
<td>不能自动选择特征</td>
<td>能自动选择特征</td>
</tr>
<tr>
<td>参数解释</td>
<td>所有参数都非零，解释复杂</td>
<td>稀疏解，解释简单</td>
</tr>
<tr>
<td>计算复杂度</td>
<td>有闭式解，计算快</td>
<td>需迭代优化，计算慢</td>
</tr>
<tr>
<td>多重共线性</td>
<td>处理效果好</td>
<td>会任意选择相关特征中的一个</td>
</tr>
<tr>
<td>预测精度</td>
<td>特征相关时表现好</td>
<td>稀疏特征时表现好</td>
</tr>
</tbody>
</table>
</div>
<h5 id="弹性网络（Elastic-Net）"><a href="#弹性网络（Elastic-Net）" class="headerlink" title="弹性网络（Elastic Net）"></a>弹性网络（Elastic Net）</h5><p>结合L1和L2正则化的优点：</p>
<script type="math/tex; mode=display">\text{Loss} = \frac{1}{2n} \lVert y - Xw \rVert_2^2 + \lambda_1 \lVert w \rVert_1 + \lambda_2 \lVert w \rVert_2^2</script><p><strong>优势</strong>：</p>
<ul>
<li>既能进行特征选择（L1），又能处理相关特征组（L2）</li>
<li>在特征数大于样本数时表现稳定</li>
<li>对相关特征组倾向于整体选择或整体剔除</li>
</ul>
<hr>
<h4 id="正则化参数λ的影响"><a href="#正则化参数λ的影响" class="headerlink" title="正则化参数λ的影响"></a>正则化参数λ的影响</h4><p><img src="/2026/01/09/MachineCollectionFinalReview/34.png" alt="34"></p>
<ul>
<li>答案：<ol>
<li>随着λ增大，<strong>Lasso</strong>回归的变量系数逐个减小为0，可以用于特征选择</li>
<li>而<strong>岭</strong>回归变量系数几乎同时趋近于0</li>
</ol>
</li>
<li>解析：<ul>
<li>Lasso回归基于L1范数正则化，其优化过程会让不重要的特征系数逐步被压缩至0，实现“逐个剔除”式的特征选择；</li>
<li>岭回归基于L2范数正则化，其正则项会让所有系数同时被缩小（但不会变为0），呈现“整体趋近于0”的特点。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="作业4-1"><a href="#作业4-1" class="headerlink" title="作业4-1"></a>作业4-1</h3><h4 id="决策树划分准则"><a href="#决策树划分准则" class="headerlink" title="决策树划分准则"></a>决策树划分准则</h4><p><img src="/2026/01/09/MachineCollectionFinalReview/35.png" alt="35"></p>
<ul>
<li>答案：正确选项是BD</li>
<li>解析：<ul>
<li>问题核心：熵/Gini会因高基数属性（如雇员id）的“过度细分”（每个划分记录数太少）而给出不合理的高纯度，导致不可靠预测。</li>
<li>选项分析：<ul>
<li>A错误：分类误差同样会受高基数属性的过度细分影响，无法解决该问题；</li>
<li>B正确：限制为二元划分（如对属性取一个阈值分成两类），可避免高基数属性的过度细分，保证每个划分有足够记录数；</li>
<li>C错误：增益指标（如信息增益）本身倾向于选择高基数属性，会加剧该问题；</li>
<li>D正确：增益率（信息增益/分裂信息）会惩罚高基数属性的过度分裂，避免选择类似雇员id的无用属性。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h4 id="Gini指数（Gini-Index）"><a href="#Gini指数（Gini-Index）" class="headerlink" title="Gini指数（Gini Index）"></a>Gini指数（Gini Index）</h4><p><img src="/2026/01/09/MachineCollectionFinalReview/36.png" alt="36"></p>
<h5 id="计算Gini的例子"><a href="#计算Gini的例子" class="headerlink" title="计算Gini的例子"></a>计算Gini的例子</h5><p><strong>Gini指数计算公式</strong>：</p>
<script type="math/tex; mode=display">GINI(t) = 1 - \sum_{j} [p(j|t)]^2</script><p>其中$p(j|t)$表示节点$t$中类别$j$的样本比例。</p>
<p><strong>示例1：完全纯净的节点</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>类别</th>
<th>样本数</th>
</tr>
</thead>
<tbody>
<tr>
<td>C1</td>
<td>0</td>
</tr>
<tr>
<td>C2</td>
<td>6</td>
</tr>
</tbody>
</table>
</div>
<p>计算过程：</p>
<ul>
<li>$P(C1) = 0/6 = 0$</li>
<li>$P(C2) = 6/6 = 1$</li>
<li>$Gini = 1 - P(C1)^2 - P(C2)^2 = 1 - 0 - 1 = 0$</li>
</ul>
<p><strong>结论</strong>：Gini=0表示节点完全纯净，所有样本属于同一类别。</p>
<p><strong>示例2：不平衡的节点</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>类别</th>
<th>样本数</th>
</tr>
</thead>
<tbody>
<tr>
<td>C1</td>
<td>1</td>
</tr>
<tr>
<td>C2</td>
<td>5</td>
</tr>
</tbody>
</table>
</div>
<p>计算过程：</p>
<ul>
<li>$P(C1) = 1/6$</li>
<li>$P(C2) = 5/6$</li>
<li>$Gini = 1 - (1/6)^2 - (5/6)^2 = 0.278$</li>
</ul>
<p><strong>示例3：较为平衡的节点</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>类别</th>
<th>样本数</th>
</tr>
</thead>
<tbody>
<tr>
<td>C1</td>
<td>2</td>
</tr>
<tr>
<td>C2</td>
<td>4</td>
</tr>
</tbody>
</table>
</div>
<p>计算过程：</p>
<ul>
<li>$P(C1) = 2/6$</li>
<li>$P(C2) = 4/6$</li>
<li>$Gini = 1 - (2/6)^2 - (4/6)^2 = 0.444$</li>
</ul>
<p><strong>Gini指数特点总结</strong>：</p>
<ul>
<li>Gini值范围：[0, 1-1/k]，其中k是类别数</li>
<li>Gini=0：节点完全纯净（最好情况）</li>
<li>Gini越大：节点越不纯净，类别分布越均匀</li>
<li>决策树构建时选择Gini减少量最大的划分方式</li>
</ul>
<hr>
<h4 id="熵（Entropy）"><a href="#熵（Entropy）" class="headerlink" title="熵（Entropy）"></a>熵（Entropy）</h4><p><strong>熵的定义</strong>：</p>
<p>熵（Entropy）是信息论中用于度量数据集纯度的指标，表示数据集的不确定性或混乱程度。</p>
<p><strong>熵的计算公式</strong>：</p>
<script type="math/tex; mode=display">Entropy(t) = -\sum_{j=0}^{c-1} p(j|t) \log_2 p(j|t)</script><p>其中：</p>
<ul>
<li>$p(j|t)$表示节点$t$中类别$j$的样本比例</li>
<li>$c$是类别总数</li>
<li>当$p(j|t) = 0$时，定义$p(j|t) \log_2 p(j|t) = 0$</li>
</ul>
<p><strong>熵的计算示例</strong>：</p>
<p><strong>示例1：完全纯净的节点</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>类别</th>
<th>样本数</th>
</tr>
</thead>
<tbody>
<tr>
<td>C1</td>
<td>0</td>
</tr>
<tr>
<td>C2</td>
<td>6</td>
</tr>
</tbody>
</table>
</div>
<p>计算过程：</p>
<ul>
<li>$P(C1) = 0/6 = 0$</li>
<li>$P(C2) = 6/6 = 1$</li>
<li>$Entropy = -0 \log 0 - 1 \log 1 = -0 - 0 = 0$</li>
</ul>
<p><strong>结论</strong>：Entropy=0表示节点完全纯净。</p>
<p><strong>示例2：不平衡的节点</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>类别</th>
<th>样本数</th>
</tr>
</thead>
<tbody>
<tr>
<td>C1</td>
<td>1</td>
</tr>
<tr>
<td>C2</td>
<td>5</td>
</tr>
</tbody>
</table>
</div>
<p>计算过程：</p>
<ul>
<li>$P(C1) = 1/6$</li>
<li>$P(C2) = 5/6$</li>
<li>$Entropy = -(1/6) \log_2 (1/6) - (5/6) \log_2 (1/6) = 0.65$</li>
</ul>
<p><strong>示例3：较为平衡的节点</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>类别</th>
<th>样本数</th>
</tr>
</thead>
<tbody>
<tr>
<td>C1</td>
<td>2</td>
</tr>
<tr>
<td>C2</td>
<td>4</td>
</tr>
</tbody>
</table>
</div>
<p>计算过程：</p>
<ul>
<li>$P(C1) = 2/6$</li>
<li>$P(C2) = 4/6$</li>
<li>$Entropy = -(2/6) \log_2 (2/6) - (4/6) \log_2 (4/6) = 0.92$</li>
</ul>
<p><strong>熵的特点总结</strong>：</p>
<ul>
<li>Entropy值范围：[0, $\log_2 c$]，其中c是类别数</li>
<li>Entropy=0：节点完全纯净（最好情况）</li>
<li>Entropy越大：节点越不纯净，类别分布越均匀</li>
<li>对于二分类问题，Entropy最大值为1（类别完全均匀时）</li>
</ul>
<hr>
<h5 id="熵与Gini指数的关系"><a href="#熵与Gini指数的关系" class="headerlink" title="熵与Gini指数的关系"></a>熵与Gini指数的关系</h5><p><strong>相似点</strong>：</p>
<ol>
<li><strong>度量目标相同</strong>：都用于衡量数据集的不纯度（impurity）</li>
<li><strong>取值范围</strong>：都在节点完全纯净时取最小值0</li>
<li><strong>单调性</strong>：都随着类别分布的均匀程度增加而增大</li>
<li><strong>应用场景</strong>：都用于决策树的节点划分选择</li>
</ol>
<p><strong>差异点</strong>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>对比维度</th>
<th>熵（Entropy）</th>
<th>Gini指数</th>
</tr>
</thead>
<tbody>
<tr>
<td>计算公式</td>
<td>$-\sum p(j&#124;t) \log_2 p(j&#124;t)$</td>
<td>$1 - \sum [p(j&#124;t)]^2$</td>
</tr>
<tr>
<td>计算复杂度</td>
<td>较高（涉及对数运算）</td>
<td>较低（仅涉及平方运算）</td>
</tr>
<tr>
<td>最大值（二分类）</td>
<td>1（类别完全均匀时）</td>
<td>0.5（类别完全均匀时）</td>
</tr>
<tr>
<td>对不纯度的敏感性</td>
<td>更敏感（对数函数变化快）</td>
<td>相对不敏感</td>
</tr>
<tr>
<td>常用算法</td>
<td>ID3、C4.5决策树</td>
<td>CART决策树</td>
</tr>
<tr>
<td>偏好特点</td>
<td>倾向于产生更平衡的树</td>
<td>倾向于将最大类别分离出来</td>
</tr>
</tbody>
</table>
</div>
<p><strong>数值对比示例</strong>（二分类情况）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>类别C1比例</th>
<th>类别C2比例</th>
<th>熵（Entropy）</th>
<th>Gini指数</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.0</td>
<td>1.0</td>
<td>0.000</td>
<td>0.000</td>
</tr>
<tr>
<td>0.1</td>
<td>0.9</td>
<td>0.469</td>
<td>0.180</td>
</tr>
<tr>
<td>0.2</td>
<td>0.8</td>
<td>0.722</td>
<td>0.320</td>
</tr>
<tr>
<td>0.3</td>
<td>0.7</td>
<td>0.881</td>
<td>0.420</td>
</tr>
<tr>
<td>0.4</td>
<td>0.6</td>
<td>0.971</td>
<td>0.480</td>
</tr>
<tr>
<td>0.5</td>
<td>0.5</td>
<td>1.000</td>
<td>0.500</td>
</tr>
</tbody>
</table>
</div>
<p><strong>几何关系</strong>：</p>
<p>对于二分类问题，设类别C1的比例为$p$，则：</p>
<ul>
<li>熵：$Entropy = -p\log_2(p) - (1-p)\log_2(1-p)$</li>
<li>Gini：$Gini = 2p(1-p) = 1 - p^2 - (1-p)^2$</li>
</ul>
<p>两者的曲线形状相似，都在$p=0.5$时达到最大值，但熵的曲线更陡峭。</p>
<p><strong>实际应用选择建议</strong>：</p>
<ul>
<li><strong>选择熵</strong>：需要更精确的不纯度度量，对类别分布变化更敏感</li>
<li><strong>选择Gini</strong>：计算效率要求高，对轻微的不纯度差异不敏感</li>
<li><strong>实践中</strong>：两者通常产生相似的决策树结构，Gini因计算简单而更常用</li>
</ul>
<hr>
<h4 id="不纯度度量：熵与分类误差"><a href="#不纯度度量：熵与分类误差" class="headerlink" title="不纯度度量：熵与分类误差"></a>不纯度度量：熵与分类误差</h4><p><img src="/2026/01/09/MachineCollectionFinalReview/37.png" alt="37"></p>
<p><strong>题目</strong>：（填空题）按照某个属性分为3类，分别有200、400、200条记录，作为该属性不纯性的度量，熵（Entropy）为<strong><em>，分类误差（classification error）为</em></strong>。</p>
<p><strong>解答过程</strong>：</p>
<p><strong>第一步：计算各类别的概率</strong></p>
<p>总记录数 = 200 + 400 + 200 = 800</p>
<ul>
<li>$P(C1) = 200/800 = 1/4 = 0.25$</li>
<li>$P(C2) = 400/800 = 1/2 = 0.5$</li>
<li>$P(C3) = 200/800 = 1/4 = 0.25$</li>
</ul>
<p><strong>第二步：计算熵（Entropy）</strong></p>
<p>熵的计算公式：</p>
<script type="math/tex; mode=display">Entropy = -\sum_{i=1}^{3} P(C_i) \log_2 P(C_i)</script><p>代入数值：</p>
<script type="math/tex; mode=display">Entropy = -P(C1) \log_2 P(C1) - P(C2) \log_2 P(C2) - P(C3) \log_2 P(C3)</script><script type="math/tex; mode=display">= -(1/4) \log_2 (1/4) - (1/2) \log_2 (1/2) - (1/4) \log_2 (1/4)</script><script type="math/tex; mode=display">= -(1/4) \times (-2) - (1/2) \times (-1) - (1/4) \times (-2)</script><script type="math/tex; mode=display">= 0.5 + 0.5 + 0.5</script><script type="math/tex; mode=display">= 1.5</script><p><strong>第三步：计算分类误差（Classification Error）</strong></p>
<p>分类误差的计算公式：</p>
<script type="math/tex; mode=display">Classification\ Error = 1 - \max_i P(C_i)</script><p>其中$\max_i P(C_i)$表示所有类别中概率最大的那个类别的概率。</p>
<p>在本题中：</p>
<ul>
<li>$P(C1) = 0.25$</li>
<li>$P(C2) = 0.5$（最大）</li>
<li>$P(C3) = 0.25$</li>
</ul>
<p>因此：</p>
<script type="math/tex; mode=display">Classification\ Error = 1 - 0.5 = 0.5</script><p><strong>答案</strong>：</p>
<ul>
<li>熵（Entropy）= <strong>1.5</strong></li>
<li>分类误差（Classification Error）= <strong>0.5</strong></li>
</ul>
<p><strong>知识点总结</strong>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>不纯度度量指标</th>
<th>计算公式</th>
<th>本题结果</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>熵（Entropy）</td>
<td>$-\sum_{i} P(C_i) \log_2 P(C_i)$</td>
<td>1.5</td>
<td>对类别分布变化敏感，常用于ID3、C4.5算法</td>
</tr>
<tr>
<td>分类误差（Classification Error）</td>
<td>$1 - \max_i P(C_i)$</td>
<td>0.5</td>
<td>计算简单，但对类别分布变化不敏感</td>
</tr>
<tr>
<td>Gini指数</td>
<td>$1 - \sum_{i} [P(C_i)]^2$</td>
<td>0.625</td>
<td>计算效率高，常用于CART算法</td>
</tr>
</tbody>
</table>
</div>
<p><strong>补充说明</strong>：</p>
<ul>
<li>对于三分类问题，熵的最大值为$\log_2 3 \approx 1.585$（当三类样本数量完全相等时）</li>
<li>本题中熵为1.5，接近最大值，说明类别分布较为均匀（虽然C2的样本数是C1和C3的两倍）</li>
<li>分类误差为0.5，表示如果将所有样本都预测为最多的类别C2，仍有50%的样本会被错误分类</li>
</ul>
<hr>
<h4 id="二元划分"><a href="#二元划分" class="headerlink" title="二元划分"></a>二元划分</h4><p><img src="/2026/01/09/MachineCollectionFinalReview/38.png" alt="38"></p>
<p><strong>题目</strong>：（填空题）决策树创建过程中，对于取值包括北京、上海、天津、重庆的籍贯属性，二元划分有<strong><em>种；对于取值包括S、M、L、XL的衣服大小属性，二元划分有</em></strong>种。</p>
<p><strong>解答过程</strong>：</p>
<p><strong>第一步：理解二元划分的概念</strong></p>
<p>二元划分（Binary Split）是指将属性的所有取值分成两个互不相交的子集，每次划分将数据分为两个分支。</p>
<p>对于有 $n$ 个不同取值的离散属性，二元划分的数量计算公式为：</p>
<script type="math/tex; mode=display">\text{二元划分数} = \frac{2^{n-1} - 1}{1} = 2^{n-1} - 1</script><p>但实际上，由于划分 $\{A\}$ 和 $\{\bar{A}\}$ 与划分 $\{\bar{A}\}$ 和 $\{A\}$ 是等价的（只是左右分支互换），所以真正不同的二元划分数为：</p>
<script type="math/tex; mode=display">\text{二元划分数} = 2^{n-1} - 1</script><p><strong>第二步：计算籍贯属性的二元划分数</strong></p>
<p>籍贯属性有4个取值：北京、上海、天津、重庆</p>
<p>方法一：使用公式</p>
<script type="math/tex; mode=display">\text{二元划分数} = 2^{4-1} - 1 = 2^3 - 1 = 8 - 1 = 7</script><p>方法二：枚举所有可能的划分<br>将4个城市分成两个非空子集的方法：</p>
<ol>
<li>$\{北京\}$ vs $\{上海, 天津, 重庆\}$</li>
<li>$\{上海\}$ vs $\{北京, 天津, 重庆\}$</li>
<li>$\{天津\}$ vs $\{北京, 上海, 重庆\}$</li>
<li>$\{重庆\}$ vs $\{北京, 上海, 天津\}$</li>
<li>$\{北京, 上海\}$ vs $\{天津, 重庆\}$</li>
<li>$\{北京, 天津\}$ vs $\{上海, 重庆\}$</li>
<li>$\{北京, 重庆\}$ vs $\{上海, 天津\}$</li>
</ol>
<p>共7种划分方式。</p>
<p><strong>第三步：计算衣服大小属性的二元划分数</strong></p>
<p>衣服大小属性有4个取值：S、M、L、XL</p>
<p><strong>重要</strong>：衣服大小是<strong>有序属性</strong>（S &lt; M &lt; L &lt; XL），对于有序属性，二元划分只能在相邻值之间进行切分，以保持顺序关系。</p>
<p>对于有 $n$ 个有序取值的属性，二元划分数为：</p>
<script type="math/tex; mode=display">\text{二元划分数} = n - 1</script><p>因此，衣服大小属性的二元划分数为：</p>
<script type="math/tex; mode=display">\text{二元划分数} = 4 - 1 = 3</script><p>枚举所有可能的划分（按顺序切分）：</p>
<ol>
<li>$\{S\}$ vs $\{M, L, XL\}$ （在S和M之间切分）</li>
<li>$\{S, M\}$ vs $\{L, XL\}$ （在M和L之间切分）</li>
<li>$\{S, M, L\}$ vs $\{XL\}$ （在L和XL之间切分）</li>
</ol>
<p>共3种划分方式。</p>
<p><strong>答案</strong>：</p>
<ul>
<li>籍贯属性的二元划分有 <strong>7</strong> 种</li>
<li>衣服大小属性的二元划分有 <strong>3</strong> 种</li>
</ul>
<p><strong>知识点总结</strong>：</p>
<p><strong>1. 无序离散属性的二元划分</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性取值数 $n$</th>
<th>二元划分数公式</th>
<th>计算结果</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>$2^{2-1} - 1$</td>
<td>1</td>
<td>性别：男/女</td>
</tr>
<tr>
<td>3</td>
<td>$2^{3-1} - 1$</td>
<td>3</td>
<td>颜色：红/绿/蓝</td>
</tr>
<tr>
<td>4</td>
<td>$2^{4-1} - 1$</td>
<td>7</td>
<td>籍贯：北京/上海/天津/重庆</td>
</tr>
<tr>
<td>5</td>
<td>$2^{5-1} - 1$</td>
<td>15</td>
<td>-</td>
</tr>
<tr>
<td>$n$</td>
<td>$2^{n-1} - 1$</td>
<td>$2^{n-1} - 1$</td>
<td>-</td>
</tr>
</tbody>
</table>
</div>
<p><strong>2. 有序离散属性的二元划分</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性取值数 $n$</th>
<th>二元划分数公式</th>
<th>计算结果</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>$n - 1$</td>
<td>1</td>
<td>温度：低/高</td>
</tr>
<tr>
<td>3</td>
<td>$n - 1$</td>
<td>2</td>
<td>等级：差/中/好</td>
</tr>
<tr>
<td>4</td>
<td>$n - 1$</td>
<td>3</td>
<td>衣服大小：S/M/L/XL</td>
</tr>
<tr>
<td>5</td>
<td>$n - 1$</td>
<td>4</td>
<td>学历：小学/初中/高中/本科/研究生</td>
</tr>
<tr>
<td>$n$</td>
<td>$n - 1$</td>
<td>$n - 1$</td>
<td>-</td>
</tr>
</tbody>
</table>
</div>
<p><strong>补充说明</strong>：</p>
<ol>
<li><p><strong>无序属性的二元划分：为什么是 $2^{n-1} - 1$？</strong></p>
<ul>
<li>将 $n$ 个元素分成两个非空子集，总共有 $2^n$ 种分配方式（每个元素可以分到左边或右边）</li>
<li>减去2种全部分到一边的情况（全左或全右），得到 $2^n - 2$</li>
<li>由于左右对称（交换左右分支是等价的），所以除以2，得到 $(2^n - 2) / 2 = 2^{n-1} - 1$</li>
<li>例如：籍贯（北京、上海、天津、重庆）是无序属性，可以任意组合划分，共7种</li>
</ul>
</li>
<li><p><strong>有序属性的二元划分：为什么是 $n-1$？</strong></p>
<ul>
<li>有序属性必须保持顺序关系，只能在相邻值之间进行切分</li>
<li>对于 $n$ 个有序值，有 $n-1$ 个相邻间隙可以作为切分点</li>
<li>例如：衣服大小 S &lt; M &lt; L &lt; XL 是有序属性，只能在3个间隙处切分：<ul>
<li>间隙1：S | M, L, XL</li>
<li>间隙2：S, M | L, XL</li>
<li>间隙3：S, M, L | XL</li>
</ul>
</li>
<li>共3种划分方式</li>
</ul>
</li>
<li><p><strong>如何判断属性是否有序？</strong></p>
<ul>
<li>有序属性：取值之间存在明确的大小或等级关系<ul>
<li>示例：温度（低&lt;中&lt;高）、学历（小学&lt;初中&lt;高中&lt;本科）、衣服尺码（S&lt;M&lt;L&lt;XL）</li>
</ul>
</li>
<li>无序属性：取值之间没有大小关系，只是类别标签<ul>
<li>示例：籍贯（北京、上海、天津、重庆）、颜色（红、绿、蓝）、性别（男、女）</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>决策树中的应用</strong>：</p>
<ul>
<li>CART算法使用二元划分构建决策树</li>
<li>对于无序属性，需要尝试所有 $2^{n-1} - 1$ 种划分，计算复杂度为 $O(2^n)$</li>
<li>对于有序属性，只需尝试 $n-1$ 种划分，计算复杂度为 $O(n)$</li>
<li>因此，识别有序属性可以显著降低决策树构建的计算成本</li>
</ul>
</li>
</ol>
<hr>
<h3 id="作业4-2"><a href="#作业4-2" class="headerlink" title="作业4-2"></a>作业4-2</h3><h4 id="Gini系数计算题"><a href="#Gini系数计算题" class="headerlink" title="Gini系数计算题"></a>Gini系数计算题</h4><p><img src="/2026/01/09/MachineCollectionFinalReview/39.png" alt="39"></p>
<p><strong>题目数据表</strong>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>顾客ID</th>
<th>性别</th>
<th>车型</th>
<th>衬衫尺码</th>
<th>类</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>男</td>
<td>家用</td>
<td>小</td>
<td>C0</td>
</tr>
<tr>
<td>2</td>
<td>男</td>
<td>运动</td>
<td>中</td>
<td>C0</td>
</tr>
<tr>
<td>3</td>
<td>男</td>
<td>运动</td>
<td>中</td>
<td>C0</td>
</tr>
<tr>
<td>4</td>
<td>男</td>
<td>运动</td>
<td>大</td>
<td>C0</td>
</tr>
<tr>
<td>5</td>
<td>男</td>
<td>运动</td>
<td>加大</td>
<td>C0</td>
</tr>
<tr>
<td>6</td>
<td>男</td>
<td>运动</td>
<td>加大</td>
<td>C0</td>
</tr>
<tr>
<td>7</td>
<td>女</td>
<td>运动</td>
<td>小</td>
<td>C0</td>
</tr>
<tr>
<td>8</td>
<td>女</td>
<td>运动</td>
<td>小</td>
<td>C0</td>
</tr>
<tr>
<td>9</td>
<td>女</td>
<td>运动</td>
<td>中</td>
<td>C0</td>
</tr>
<tr>
<td>10</td>
<td>女</td>
<td>豪华</td>
<td>大</td>
<td>C0</td>
</tr>
<tr>
<td>11</td>
<td>男</td>
<td>家用</td>
<td>大</td>
<td>C1</td>
</tr>
<tr>
<td>12</td>
<td>男</td>
<td>家用</td>
<td>加大</td>
<td>C1</td>
</tr>
<tr>
<td>13</td>
<td>男</td>
<td>家用</td>
<td>中</td>
<td>C1</td>
</tr>
<tr>
<td>14</td>
<td>男</td>
<td>豪华</td>
<td>加大</td>
<td>C1</td>
</tr>
<tr>
<td>15</td>
<td>女</td>
<td>豪华</td>
<td>小</td>
<td>C1</td>
</tr>
<tr>
<td>16</td>
<td>女</td>
<td>豪华</td>
<td>小</td>
<td>C1</td>
</tr>
<tr>
<td>17</td>
<td>女</td>
<td>豪华</td>
<td>中</td>
<td>C1</td>
</tr>
<tr>
<td>18</td>
<td>女</td>
<td>豪华</td>
<td>中</td>
<td>C1</td>
</tr>
<tr>
<td>19</td>
<td>女</td>
<td>豪华</td>
<td>中</td>
<td>C1</td>
</tr>
<tr>
<td>20</td>
<td>女</td>
<td>豪华</td>
<td>大</td>
<td>C1</td>
</tr>
</tbody>
</table>
</div>
<p><strong>题目要求</strong>：</p>
<ol>
<li>计算整个数据集的Gini指标值</li>
<li>计算属性性别的Gini指标值</li>
<li>计算使用多路划分属性车型的Gini指标值</li>
<li>决策树中使用下面哪个属性更好：性别、车型还是衬衫尺码？为什么？</li>
</ol>
<hr>
<p><strong>解答过程</strong>：</p>
<p>（1）计算整个数据集的Gini指标值</p>
<p><strong>第一步：统计类别分布</strong></p>
<p>总样本数 = 20</p>
<ul>
<li>C0类样本数 = 10（顾客1-10）</li>
<li>C1类样本数 = 10（顾客11-20）</li>
</ul>
<p><strong>第二步：计算概率</strong></p>
<ul>
<li>$P(C0) = 10/20 = 0.5$</li>
<li>$P(C1) = 10/20 = 0.5$</li>
</ul>
<p><strong>第三步：计算Gini指数</strong></p>
<script type="math/tex; mode=display">Gini = 1 - \sum_{i} [P(C_i)]^2</script><script type="math/tex; mode=display">Gini = 1 - [P(C0)]^2 - [P(C1)]^2</script><script type="math/tex; mode=display">= 1 - (0.5)^2 - (0.5)^2</script><script type="math/tex; mode=display">= 1 - 0.25 - 0.25</script><script type="math/tex; mode=display">= 0.5</script><p><strong>答案</strong>：整个数据集的Gini指标值 = <strong>0.5</strong></p>
<hr>
<p>（2）计算属性性别的Gini指标值</p>
<p><strong>第一步：按性别划分数据</strong></p>
<p><strong>男性样本（共10个）</strong>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>顾客ID</th>
<th>类别</th>
</tr>
</thead>
<tbody>
<tr>
<td>1, 2, 3, 4, 5, 6</td>
<td>C0</td>
</tr>
<tr>
<td>11, 12, 13, 14</td>
<td>C1</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>男性中C0类：6个</li>
<li>男性中C1类：4个</li>
</ul>
<p><strong>女性样本（共10个）</strong>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>顾客ID</th>
<th>类别</th>
</tr>
</thead>
<tbody>
<tr>
<td>7, 8, 9, 10</td>
<td>C0</td>
</tr>
<tr>
<td>15, 16, 17, 18, 19, 20</td>
<td>C1</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>女性中C0类：4个</li>
<li>女性中C1类：6个</li>
</ul>
<p><strong>第二步：计算男性子集的Gini</strong></p>
<script type="math/tex; mode=display">P(C0|男) = 6/10 = 3/5</script><script type="math/tex; mode=display">P(C1|男) = 4/10 = 2/5</script><script type="math/tex; mode=display">Gini(男) = 1 - (6/10)^2 - (4/10)^2</script><script type="math/tex; mode=display">= 1 - 36/100 - 16/100</script><script type="math/tex; mode=display">= 1 - 52/100</script><script type="math/tex; mode=display">= 48/100 = 0.48</script><p><strong>第三步：计算女性子集的Gini</strong></p>
<script type="math/tex; mode=display">P(C0|女) = 4/10 = 2/5</script><script type="math/tex; mode=display">P(C1|女) = 6/10 = 3/5</script><script type="math/tex; mode=display">Gini(女) = 1 - (4/10)^2 - (6/10)^2</script><script type="math/tex; mode=display">= 1 - 16/100 - 36/100</script><script type="math/tex; mode=display">= 1 - 52/100</script><script type="math/tex; mode=display">= 48/100 = 0.48</script><p><strong>第四步：计算加权平均Gini</strong></p>
<script type="math/tex; mode=display">Gini(性别) = \frac{10}{20} \times Gini(男) + \frac{10}{20} \times Gini(女)</script><script type="math/tex; mode=display">= \frac{10}{20} \times 0.48 + \frac{10}{20} \times 0.48</script><script type="math/tex; mode=display">= 0.24 + 0.24</script><script type="math/tex; mode=display">= 0.48</script><p><strong>答案</strong>：属性性别的Gini指标值 = <strong>0.48</strong></p>
<hr>
<p>（3）计算使用多路划分属性车型的Gini指标值</p>
<p><strong>第一步：按车型划分数据</strong></p>
<p><strong>家用车样本（共4个）</strong>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>顾客ID</th>
<th>类别</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>C0</td>
</tr>
<tr>
<td>11, 12, 13</td>
<td>C1</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>家用车中C0类：1个</li>
<li>家用车中C1类：3个</li>
</ul>
<p><strong>运动车样本（共8个）</strong>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>顾客ID</th>
<th>类别</th>
</tr>
</thead>
<tbody>
<tr>
<td>2, 3, 4, 5, 6, 7, 8, 9</td>
<td>C0</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>运动车中C0类：8个</li>
<li>运动车中C1类：0个</li>
</ul>
<p><strong>豪华车样本（共8个）</strong>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>顾客ID</th>
<th>类别</th>
</tr>
</thead>
<tbody>
<tr>
<td>10</td>
<td>C0</td>
</tr>
<tr>
<td>14, 15, 16, 17, 18, 19, 20</td>
<td>C1</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>豪华车中C0类：1个</li>
<li>豪华车中C1类：7个</li>
</ul>
<p><strong>第二步：计算各子集的Gini</strong></p>
<p><strong>家用车的Gini</strong>：</p>
<script type="math/tex; mode=display">Gini(家用) = 1 - (1/4)^2 - (3/4)^2</script><script type="math/tex; mode=display">= 1 - 1/16 - 9/16</script><script type="math/tex; mode=display">= 1 - 10/16</script><script type="math/tex; mode=display">= 6/16 = 0.375</script><p><strong>运动车的Gini</strong>：</p>
<script type="math/tex; mode=display">Gini(运动) = 1 - (8/8)^2 - (0/8)^2</script><script type="math/tex; mode=display">= 1 - 1 - 0</script><script type="math/tex; mode=display">= 0</script><p><strong>豪华车的Gini</strong>：</p>
<script type="math/tex; mode=display">Gini(豪华) = 1 - (1/8)^2 - (7/8)^2</script><script type="math/tex; mode=display">= 1 - 1/64 - 49/64</script><script type="math/tex; mode=display">= 1 - 50/64</script><script type="math/tex; mode=display">= 14/64 = 7/32 \approx 0.219</script><p><strong>第三步：计算加权平均Gini</strong></p>
<script type="math/tex; mode=display">Gini(车型) = \frac{4}{20} \times Gini(家用) + \frac{8}{20} \times Gini(运动) + \frac{8}{20} \times Gini(豪华)</script><script type="math/tex; mode=display">= \frac{4}{20} \times 0.375 + \frac{8}{20} \times 0 + \frac{8}{20} \times 0.219</script><script type="math/tex; mode=display">= 0.075 + 0 + 0.088</script><script type="math/tex; mode=display">= 0.163</script><p><strong>答案</strong>：属性车型的Gini指标值 = <strong>0.163</strong></p>
<hr>
<p>（4）决策树中使用下面哪个属性更好：性别、车型还是衬衫尺码？为什么？</p>
<p><strong>第一步：计算衬衫尺码的Gini指标值</strong></p>
<p><strong>按衬衫尺码划分数据</strong>：</p>
<p><strong>小号样本（共5个）</strong>：</p>
<ul>
<li>C0类：3个（顾客1, 7, 8）</li>
<li>C1类：2个（顾客15, 16）</li>
</ul>
<script type="math/tex; mode=display">Gini(小) = 1 - (3/5)^2 - (2/5)^2 = 1 - 9/25 - 4/25 = 12/25 = 0.48</script><p><strong>中号样本（共6个）</strong>：</p>
<ul>
<li>C0类：3个（顾客2, 3, 9）<br><strong>中号样本（共7个）</strong>：</li>
<li>C0类：3个（顾客2, 3, 9）</li>
<li>C1类：4个（顾客13, 17, 18, 19）</li>
</ul>
<script type="math/tex; mode=display">Gini(中) = 1 - (3/7)^2 - (4/7)^2 = 1 - 9/49 - 16/49 = 24/49 \approx 0.490</script><p><strong>大号样本（共4个）</strong>：</p>
<ul>
<li>C0类：2个（顾客4, 10）</li>
<li>C1类：2个（顾客11, 20）</li>
</ul>
<script type="math/tex; mode=display">Gini(大) = 1 - (2/4)^2 - (2/4)^2 = 1 - 0.25 - 0.25 = 0.5</script><p><strong>加大号样本（共4个）</strong>：</p>
<ul>
<li>C0类：2个（顾客5, 6）</li>
<li>C1类：2个（顾客12, 14）</li>
</ul>
<script type="math/tex; mode=display">Gini(加大) = 1 - (2/4)^2 - (2/4)^2 = 0.5</script><p><strong>加权平均Gini</strong>：</p>
<script type="math/tex; mode=display">Gini(衬衫尺码) = \frac{5}{20} \times 0.48 + \frac{7}{20} \times 0.490 + \frac{4}{20} \times 0.5 + \frac{4}{20} \times 0.5</script><script type="math/tex; mode=display">= 0.12 + 0.172 + 0.1 + 0.1</script><script type="math/tex; mode=display">= 0.492</script><p><strong>第二步：比较各属性的Gini指标值</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>Gini指标值</th>
<th>Gini减少量</th>
</tr>
</thead>
<tbody>
<tr>
<td>原始数据集</td>
<td>0.500</td>
<td>-</td>
</tr>
<tr>
<td>性别</td>
<td>0.480</td>
<td>0.020</td>
</tr>
<tr>
<td>车型</td>
<td>0.163</td>
<td>0.337</td>
</tr>
<tr>
<td>衬衫尺码</td>
<td>0.492</td>
<td>0.008</td>
</tr>
</tbody>
</table>
</div>
<p><strong>第三步：得出结论</strong></p>
<p><strong>答案</strong>：<strong>车型</strong>属性更好</p>
<p><strong>原因</strong>：</p>
<ol>
<li><p><strong>Gini减少量最大</strong>：车型属性的Gini指标值为0.256，相比原始数据集的0.5，减少了0.244，是三个属性中减少量最大的。</p>
</li>
<li><p><strong>不纯度降低最明显</strong>：Gini指标值越小，表示数据集越纯净。车型属性划分后的Gini值最小，说明按车型划分能最有效地将不同类别分开。</p>
</li>
<li><p><strong>信息增益最大</strong>：Gini减少量可以理解为信息增益，车型属性提供了最多的信息来区分C0和C1类别。</p>
</li>
<li><p><strong>具体分析</strong>：</p>
<ul>
<li>运动车全是C0类（8/8），纯度完美（Gini=0）</li>
<li>豪华车主要是C1类（7/8），纯度很高</li>
<li>家用车相对混合（1个C0，3个C1）</li>
<li>这种划分使得各子集的类别分布更加集中，便于分类</li>
</ul>
</li>
</ol>
<p><strong>排序</strong>：车型（0.163）&gt; 性别（0.480）&gt; 衬衫尺码（0.492）</p>
<p>因此，在决策树构建时，应该优先选择<strong>车型</strong>作为根节点的划分属性。</p>
<hr>
<p><strong>知识点总结</strong>：</p>
<ol>
<li><p><strong>Gini指数的计算</strong>：$Gini = 1 - \sum_{i} [P(C_i)]^2$</p>
</li>
<li><p><strong>属性划分的Gini计算</strong>：加权平均各子集的Gini值</p>
</li>
<li><p><strong>属性选择准则</strong>：选择Gini指标值最小（或Gini减少量最大）的属性</p>
</li>
<li><p><strong>决策树构建原则</strong>：每次选择能最大程度降低不纯度的属性进行划分</p>
</li>
</ol>
<hr>
<h3 id="作业5"><a href="#作业5" class="headerlink" title="作业5"></a>作业5</h3><h4 id="K近邻分类器（KNN）"><a href="#K近邻分类器（KNN）" class="headerlink" title="K近邻分类器（KNN）"></a>K近邻分类器（KNN）</h4><p><img src="/2026/01/09/MachineCollectionFinalReview/40.png" alt="40"></p>
<p><strong>题目</strong>：下图是一个K近邻分类器，x表示待分类样本，”+”和”-“分别表示训练集中正负样本。</p>
<ol>
<li>当K选值为1、2、3时，待分类样本x分别被判断为哪个类别？</li>
<li>K值选择对K近邻分类有什么影响？</li>
</ol>
<p><strong>解答过程</strong>：</p>
<p>（1）当K选值为1、2、3时，待分类样本x分别被判断为哪个类别？</p>
<p><strong>K近邻算法（KNN）的分类原则</strong>：</p>
<p>找到距离待分类样本x最近的K个训练样本，统计这K个样本中各类别的数量，将x分类为数量最多的类别。</p>
<p><strong>分析三幅图</strong>：</p>
<p><strong>图1：K=1</strong></p>
<ul>
<li>红色虚线圆表示距离x最近的1个样本的范围</li>
<li>圆内最近的1个样本是：1个”-“（负样本）</li>
<li>统计结果：”-“类1个，”+”类0个</li>
<li><strong>分类结果</strong>：x被判断为<strong>“-“类（负类）</strong></li>
</ul>
<p><strong>图2：K=2</strong></p>
<ul>
<li>红色虚线圆表示距离x最近的2个样本的范围</li>
<li>圆内最近的2个样本是：1个”-“（负样本）+ 1个”+”（正样本）</li>
<li>统计结果：”-“类1个，”+”类1个</li>
<li>出现平局时的处理：<ul>
<li>方法1：选择距离更近的样本的类别</li>
<li>方法2：随机选择一个类别</li>
<li>方法3：选择K=1时的结果</li>
</ul>
</li>
<li><strong>分类结果</strong>：根据最近邻原则，x被判断为<strong>“-“类（负类）</strong></li>
</ul>
<p><strong>图3：K=3</strong></p>
<ul>
<li>红色虚线圆表示距离x最近的3个样本的范围</li>
<li>圆内最近的3个样本是：1个”-“（负样本）+ 2个”+”（正样本）</li>
<li>统计结果：”-“类1个，”+”类2个</li>
<li><strong>分类结果</strong>：x被判断为<strong>“+”类（正类）</strong></li>
</ul>
<p><strong>答案总结</strong>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>K值</th>
<th>最近邻样本统计</th>
<th>分类结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>K=1</td>
<td>“-“类：1个，”+”类：0个</td>
<td><strong>“-“类（负类）</strong></td>
</tr>
<tr>
<td>K=2</td>
<td>“-“类：1个，”+”类：1个</td>
<td><strong>“-“类（负类）</strong>（平局时选最近邻）</td>
</tr>
<tr>
<td>K=3</td>
<td>“-“类：1个，”+”类：2个</td>
<td><strong>“+”类（正类）</strong></td>
</tr>
</tbody>
</table>
</div>
<hr>
<p>（2）K值选择对K近邻分类有什么影响？</p>
<p><strong>K值对KNN分类的影响</strong>：</p>
<p><strong>1. K值较小（如K=1）</strong></p>
<p><strong>优点</strong>：</p>
<ul>
<li>模型复杂度高，对训练数据拟合度好</li>
<li>能够捕捉数据的局部特征</li>
<li>对近邻点非常敏感</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>容易受噪声影响，抗噪声能力差</li>
<li>容易过拟合</li>
<li>预测结果不稳定，方差大</li>
<li>如果最近的样本是噪声点，会导致错误分类</li>
</ul>
<p><strong>2. K值较大（如K=10或更大）</strong></p>
<p><strong>优点</strong>：</p>
<ul>
<li>模型简化，减少过拟合风险</li>
<li>抗噪声能力强</li>
<li>预测结果稳定，方差小</li>
<li>决策边界更加平滑</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>容易欠拟合</li>
<li>忽略数据的局部特征</li>
<li>可能将不同类别的样本混在一起</li>
<li>计算量增大</li>
</ul>
<p><strong>为什么K值大会导致欠拟合？</strong></p>
<p>欠拟合是指模型过于简单，无法捕捉数据的真实模式。当K值很大时：</p>
<ol>
<li><p><strong>过度平滑</strong>：考虑了太多远距离的样本，导致决策边界过于平滑，无法反映数据的真实分布</p>
<ul>
<li>例如：在类别边界附近的样本，本应根据附近的样本判断类别</li>
<li>但K值过大时，会把远处不相关的样本也纳入投票，稀释了局部特征</li>
</ul>
</li>
<li><p><strong>多数类主导</strong>：当K值接近样本总数时，分类结果会趋向于训练集中数量最多的类别</p>
<ul>
<li>极端情况：K=N（总样本数），所有待分类样本都会被判为训练集中的多数类</li>
<li>这相当于忽略了样本的位置信息，只看整体类别分布</li>
</ul>
</li>
<li><p><strong>丢失局部信息</strong>：数据的真实分布往往具有局部性（相似的样本聚集在一起）</p>
<ul>
<li>K值过大会把不同区域的样本混在一起</li>
<li>无法区分局部的细微差异</li>
</ul>
</li>
</ol>
<p><strong>举例说明</strong>：</p>
<p>假设有一个二分类问题：</p>
<ul>
<li>训练集中有70个”+”类样本，30个”-“类样本</li>
<li>待分类样本x周围3个最近邻都是”-“类</li>
<li>但如果K=50，可能会包含35个”+”类和15个”-“类</li>
<li>结果x被错误分类为”+”类，因为忽略了x的局部特征</li>
</ul>
<p><strong>形象比喻</strong>：</p>
<ul>
<li>K值小：像”近视眼”，只看得清眼前的东西（局部特征），容易被噪声干扰</li>
<li>K值大：像”远视眼”，只看得清远处的整体（全局特征），看不清局部细节</li>
<li>K值适中：视力正常，既能看清局部，又不会被噪声迷惑</li>
</ul>
<p><strong>3. K值适中</strong></p>
<ul>
<li>在偏差和方差之间取得平衡</li>
<li>既能保持对局部特征的敏感性，又能抵抗噪声</li>
<li>通常通过交叉验证选择最优K值</li>
</ul>
<p><strong>K值选择的一般原则</strong>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>考虑因素</th>
<th>建议</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据集大小</td>
<td>数据集越大，K值可以适当增大</td>
</tr>
<tr>
<td>噪声水平</td>
<td>噪声越多，K值应该越大</td>
</tr>
<tr>
<td>类别分布</td>
<td>类别不平衡时，K值不宜过大</td>
</tr>
<tr>
<td>计算资源</td>
<td>K值越大，计算量越大</td>
</tr>
<tr>
<td>经验法则</td>
<td>K通常取奇数（避免平局），常用值：3、5、7、9</td>
</tr>
<tr>
<td>最优选择</td>
<td>通过交叉验证确定最优K值</td>
</tr>
</tbody>
</table>
</div>
<p><strong>K值与决策边界的关系</strong>：</p>
<ul>
<li><strong>K=1</strong>：决策边界非常复杂，呈现锯齿状，容易过拟合</li>
<li><strong>K值增大</strong>：决策边界逐渐平滑，模型趋于简化</li>
<li><strong>K=N（样本总数）</strong>：所有样本都参与投票，相当于将所有样本分为训练集中数量最多的类别</li>
</ul>
<p><strong>实际应用建议</strong>：</p>
<ol>
<li><strong>起始值</strong>：通常从K=3或K=5开始尝试</li>
<li><strong>交叉验证</strong>：使用K折交叉验证选择最优K值</li>
<li><strong>奇数优先</strong>：选择奇数K值可以避免二分类问题中的平局</li>
<li><strong>经验公式</strong>：$K \approx \sqrt{N}$（N为训练样本数），但需要根据实际情况调整</li>
<li><strong>性能评估</strong>：在验证集上测试不同K值的分类准确率，选择最优值</li>
</ol>
<p><strong>本题示例的启示</strong>：</p>
<p>从本题可以看出，K值从1变化到3，分类结果从”-“类变为”+”类，说明：</p>
<ul>
<li>K值的选择直接影响分类结果</li>
<li>较小的K值更容易受到局部样本分布的影响</li>
<li>需要根据具体问题和数据特点选择合适的K值</li>
<li>不存在”万能”的K值，需要通过实验确定</li>
</ul>
<hr>
<p><strong>知识点总结</strong>：</p>
<p><strong>KNN算法的核心要素</strong>：</p>
<ol>
<li><strong>距离度量</strong>：常用欧氏距离、曼哈顿距离、闵可夫斯基距离</li>
<li><strong>K值选择</strong>：影响模型的复杂度和泛化能力</li>
<li><strong>决策规则</strong>：多数投票（分类）或平均值（回归）</li>
</ol>
<p><strong>KNN算法的优缺点</strong>：</p>
<p><strong>优点</strong>：</p>
<ul>
<li>算法简单，易于理解和实现</li>
<li>无需训练过程，属于懒惰学习（Lazy Learning）</li>
<li>对异常值不敏感（当K值较大时）</li>
<li>适用于多分类问题</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>计算量大，预测时需要计算与所有训练样本的距离</li>
<li>内存开销大，需要存储所有训练样本</li>
<li>对样本不平衡问题敏感</li>
<li>对高维数据效果不佳（维度灾难）</li>
<li>需要选择合适的K值和距离度量</li>
</ul>
<hr>
<h4 id="贝叶斯分类法"><a href="#贝叶斯分类法" class="headerlink" title="贝叶斯分类法"></a>贝叶斯分类法</h4><p><strong>贝叶斯分类法的基本原理</strong>：</p>
<p>贝叶斯分类是基于贝叶斯定理的统计学分类方法，通过计算样本属于各个类别的概率，选择概率最大的类别作为预测结果。</p>
<p><strong>贝叶斯定理</strong>：</p>
<script type="math/tex; mode=display">P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}</script><p>其中：</p>
<ul>
<li>$P(C|X)$：后验概率，在观察到特征X的条件下，样本属于类别C的概率</li>
<li>$P(X|C)$：似然概率，在类别C的条件下，出现特征X的概率</li>
<li>$P(C)$：先验概率，类别C在训练集中出现的概率</li>
<li>$P(X)$：证据因子，特征X出现的概率（对所有类别相同，可省略）</li>
</ul>
<p><strong>朴素贝叶斯分类器</strong>：</p>
<p>朴素贝叶斯假设各特征之间相互独立，因此：</p>
<script type="math/tex; mode=display">P(X|C) = P(x_1, x_2, ..., x_n|C) = P(x_1|C) \cdot P(x_2|C) \cdot ... \cdot P(x_n|C)</script><p><strong>分类决策规则</strong>：</p>
<p>对于待分类样本X，计算其属于每个类别的后验概率，选择概率最大的类别：</p>
<script type="math/tex; mode=display">\hat{C} = \arg\max_{C} P(C|X) = \arg\max_{C} P(C) \cdot \prod_{i=1}^{n} P(x_i|C)</script><p><strong>贝叶斯分类的步骤</strong>：</p>
<ol>
<li><strong>计算先验概率</strong> $P(C)$：统计训练集中各类别的样本比例</li>
<li><strong>计算条件概率</strong> $P(x_i|C)$：统计在类别C下，各特征取值的概率</li>
<li><strong>计算后验概率</strong> $P(C|X)$：应用贝叶斯公式</li>
<li><strong>做出预测</strong>：选择后验概率最大的类别</li>
</ol>
<p><strong>贝叶斯分类法的优缺点</strong>：</p>
<p><strong>优点</strong>：</p>
<ul>
<li>算法简单，易于实现</li>
<li>对小规模数据表现良好</li>
<li>对缺失数据不敏感</li>
<li>可以处理多分类问题</li>
<li>训练速度快</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>假设特征独立，实际中往往不成立</li>
<li>对输入数据的表达形式敏感</li>
<li>需要计算先验概率，分类决策存在错误率</li>
</ul>
<hr>
<p><img src="/2026/01/09/MachineCollectionFinalReview/41.png" alt="41"></p>
<p><strong>题目</strong>：下图是一组人的身高、体重、鞋码属性和性别标签，某人的身高为”高”、体重为”中”，鞋码为”中”，运用贝叶斯分类方法，判断这个人最可能的性别是什么。</p>
<p><strong>训练数据表</strong>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>编号</th>
<th>身高</th>
<th>体重</th>
<th>鞋码</th>
<th>性别</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>高</td>
<td>重</td>
<td>大</td>
<td>男</td>
</tr>
<tr>
<td>2</td>
<td>高</td>
<td>重</td>
<td>大</td>
<td>男</td>
</tr>
<tr>
<td>3</td>
<td>中</td>
<td>中</td>
<td>大</td>
<td>男</td>
</tr>
<tr>
<td>4</td>
<td>中</td>
<td>中</td>
<td>中</td>
<td>男</td>
</tr>
<tr>
<td>5</td>
<td>矮</td>
<td>轻</td>
<td>小</td>
<td>女</td>
</tr>
<tr>
<td>6</td>
<td>矮</td>
<td>轻</td>
<td>小</td>
<td>女</td>
</tr>
<tr>
<td>7</td>
<td>矮</td>
<td>中</td>
<td>中</td>
<td>女</td>
</tr>
<tr>
<td>8</td>
<td>中</td>
<td>中</td>
<td>中</td>
<td>女</td>
</tr>
</tbody>
</table>
</div>
<p><strong>待分类样本</strong>：身高=”高”，体重=”中”，鞋码=”中”</p>
<details class="toggle"><summary class="toggle-button" style>超绝无敌手写版题解</summary><div class="toggle-content"><p><img src="/2026/01/09/MachineCollectionFinalReview/42.png" alt="42"></p>
</div></details>
<hr>
<p><strong>解答过程</strong>：</p>
<p><strong>第一步：计算先验概率 P(性别)</strong></p>
<p>统计训练集中各性别的样本数：</p>
<ul>
<li>男性样本数：4个（编号1, 2, 3, 4）</li>
<li>女性样本数：4个（编号5, 6, 7, 8）</li>
<li>总样本数：8个</li>
</ul>
<p>先验概率：</p>
<ul>
<li>$P(男) = 4/8 = 0.5$</li>
<li>$P(女) = 4/8 = 0.5$</li>
</ul>
<hr>
<p><strong>第二步：计算条件概率 P(特征|性别)</strong></p>
<p><strong>对于男性（4个样本）</strong>：</p>
<p><strong>身高属性</strong>：</p>
<ul>
<li>$P(身高=高|男) = 2/4 = 0.5$（编号1, 2）</li>
<li>$P(身高=中|男) = 2/4 = 0.5$（编号3, 4）</li>
<li>$P(身高=矮|男) = 0/4 = 0$</li>
</ul>
<p><strong>体重属性</strong>：</p>
<ul>
<li>$P(体重=重|男) = 2/4 = 0.5$（编号1, 2）</li>
<li>$P(体重=中|男) = 2/4 = 0.5$（编号3, 4）</li>
<li>$P(体重=轻|男) = 0/4 = 0$</li>
</ul>
<p><strong>鞋码属性</strong>：</p>
<ul>
<li>$P(鞋码=大|男) = 3/4 = 0.75$（编号1, 2, 3）</li>
<li>$P(鞋码=中|男) = 1/4 = 0.25$（编号4）</li>
<li>$P(鞋码=小|男) = 0/4 = 0$</li>
</ul>
<p><strong>对于女性（4个样本）</strong>：</p>
<p><strong>身高属性</strong>：</p>
<ul>
<li>$P(身高=高|女) = 0/4 = 0$</li>
<li>$P(身高=中|女) = 1/4 = 0.25$（编号8）</li>
<li>$P(身高=矮|女) = 3/4 = 0.75$（编号5, 6, 7）</li>
</ul>
<p><strong>体重属性</strong>：</p>
<ul>
<li>$P(体重=重|女) = 0/4 = 0$</li>
<li>$P(体重=中|女) = 2/4 = 0.5$（编号7, 8）</li>
<li>$P(体重=轻|女) = 2/4 = 0.5$（编号5, 6）</li>
</ul>
<p><strong>鞋码属性</strong>：</p>
<ul>
<li>$P(鞋码=大|女) = 0/4 = 0$</li>
<li>$P(鞋码=中|女) = 2/4 = 0.5$（编号7, 8）</li>
<li>$P(鞋码=小|女) = 2/4 = 0.5$（编号5, 6）</li>
</ul>
<hr>
<p><strong>第三步：计算后验概率 P(性别|特征)</strong></p>
<p>根据朴素贝叶斯假设，特征之间相互独立：</p>
<p><strong>对于男性</strong>：</p>
<script type="math/tex; mode=display">P(男|高,中,中) = P(男) \times P(身高=高|男) \times P(体重=中|男) \times P(鞋码=中|男)</script><script type="math/tex; mode=display">= 0.5 \times 0.5 \times 0.5 \times 0.25</script><script type="math/tex; mode=display">= 0.03125</script><p><strong>对于女性</strong>：</p>
<script type="math/tex; mode=display">P(女|高,中,中) = P(女) \times P(身高=高|女) \times P(体重=中|女) \times P(鞋码=中|女)</script><script type="math/tex; mode=display">= 0.5 \times 0 \times 0.5 \times 0.5</script><script type="math/tex; mode=display">= 0</script><hr>
<p><strong>第四步：做出预测</strong></p>
<p>比较后验概率：</p>
<ul>
<li>$P(男|高,中,中) = 0.03125$</li>
<li>$P(女|高,中,中) = 0$</li>
</ul>
<p>由于 $P(男|高,中,中) &gt; P(女|高,中,中)$</p>
<p><strong>答案</strong>：这个人最可能的性别是<strong>男性</strong></p>
<hr>
<p><strong>分析说明</strong>：</p>
<ol>
<li><p><strong>为什么女性概率为0？</strong></p>
<ul>
<li>在训练集中，没有身高为”高”的女性样本</li>
<li>因此 $P(身高=高|女) = 0$</li>
<li>导致整个后验概率为0</li>
</ul>
</li>
<li><p><strong>零概率问题</strong>：</p>
<ul>
<li>这是朴素贝叶斯的一个缺陷：如果某个特征值在训练集中未出现，会导致整个概率为0</li>
<li>实际应用中通常使用<strong>拉普拉斯平滑</strong>（Laplace Smoothing）来解决</li>
</ul>
</li>
<li><p><strong>拉普拉斯平滑</strong>：</p>
<ul>
<li>在计算条件概率时，给每个特征值的计数加1</li>
<li>公式：$P(x_i|C) = \frac{count(x_i, C) + 1}{count(C) + k}$</li>
<li>其中k是该特征的取值数量</li>
</ul>
</li>
</ol>
<p><strong>使用拉普拉斯平滑重新计算</strong>：</p>
<p><strong>对于女性（使用平滑）</strong>：</p>
<ul>
<li>$P(身高=高|女) = \frac{0+1}{4+3} = \frac{1}{7} \approx 0.143$</li>
<li>$P(体重=中|女) = \frac{2+1}{4+3} = \frac{3}{7} \approx 0.429$</li>
<li>$P(鞋码=中|女) = \frac{2+1}{4+3} = \frac{3}{7} \approx 0.429$</li>
</ul>
<script type="math/tex; mode=display">P(女|高,中,中) = 0.5 \times 0.143 \times 0.429 \times 0.429 \approx 0.0131</script><p><strong>对于男性（使用平滑）</strong>：</p>
<ul>
<li>$P(身高=高|男) = \frac{2+1}{4+3} = \frac{3}{7} \approx 0.429$</li>
<li>$P(体重=中|男) = \frac{2+1}{4+3} = \frac{3}{7} \approx 0.429$</li>
<li>$P(鞋码=中|男) = \frac{1+1}{4+3} = \frac{2}{7} \approx 0.286$</li>
</ul>
<script type="math/tex; mode=display">P(男|高,中,中) = 0.5 \times 0.429 \times 0.429 \times 0.286 \approx 0.0263</script><p>使用拉普拉斯平滑后，结论仍然是<strong>男性</strong>（0.0263 &gt; 0.0131）</p>
<hr>
<p><strong>知识点总结</strong>：</p>
<p><strong>贝叶斯分类的关键点</strong>：</p>
<ol>
<li><strong>先验概率</strong>：反映各类别在训练集中的分布</li>
<li><strong>条件概率</strong>：反映特征在各类别中的分布</li>
<li><strong>独立性假设</strong>：简化计算，但可能不符合实际</li>
<li><strong>零概率问题</strong>：需要使用平滑技术处理</li>
</ol>
<p><strong>贝叶斯分类与其他方法的对比</strong>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>对比维度</th>
<th>贝叶斯分类</th>
<th>KNN</th>
<th>决策树</th>
</tr>
</thead>
<tbody>
<tr>
<td>理论基础</td>
<td>概率论</td>
<td>距离度量</td>
<td>信息论</td>
</tr>
<tr>
<td>训练速度</td>
<td>快</td>
<td>无需训练</td>
<td>中等</td>
</tr>
<tr>
<td>预测速度</td>
<td>快</td>
<td>慢</td>
<td>快</td>
</tr>
<tr>
<td>可解释性</td>
<td>中等</td>
<td>差</td>
<td>好</td>
</tr>
<tr>
<td>对噪声敏感度</td>
<td>低</td>
<td>高（K小时）</td>
<td>中等</td>
</tr>
<tr>
<td>特征独立性要求</td>
<td>高</td>
<td>无</td>
<td>无</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="作业6"><a href="#作业6" class="headerlink" title="作业6"></a>作业6</h3><h4 id="Kmeans与DBSCAN聚类算法对比"><a href="#Kmeans与DBSCAN聚类算法对比" class="headerlink" title="Kmeans与DBSCAN聚类算法对比"></a>Kmeans与DBSCAN聚类算法对比</h4><p><img src="/2026/01/09/MachineCollectionFinalReview/43.png" alt="43"></p>
<p><strong>题目</strong>：</p>
<ol>
<li>下图是Kmeans和DBSCAN两种聚类的结果，同一种颜色表示一个簇。<ul>
<li>(1) 结果(a)和(b)最可能是用什么聚类方法的结果，说明理由</li>
<li>(2) 如果Kmeans和DBSCAN聚类效果接近，要从中选择聚类算法运算速度最快的，应该选哪一种，说明理由</li>
</ul>
</li>
</ol>
<hr>
<p><strong>答案与解析</strong>：</p>
<p><strong>问题(1)：结果(a)和(b)最可能是用什么聚类方法的结果，说明理由</strong></p>
<p><strong>答案</strong>：</p>
<ul>
<li>结果(a)是<strong>Kmeans</strong>的聚类结果</li>
<li>结果(b)是<strong>DBSCAN</strong>的聚类结果</li>
</ul>
<p><strong>理由</strong>：</p>
<p><strong>结果(a)为Kmeans的特征</strong>：</p>
<ol>
<li><strong>簇的形状规则</strong>：三个簇的形状都比较规则，呈现近似圆形或椭圆形的分布</li>
<li><strong>簇的大小相近</strong>：三个簇的样本数量和空间范围大致相当</li>
<li><strong>边界清晰但不自然</strong>：在三个簇的交界处，边界是直线或平滑曲线，这是因为Kmeans基于距离划分，每个点被分配到最近的簇中心</li>
<li><strong>所有点都被分类</strong>：图中所有数据点都被分配到某个簇中，没有噪声点或离群点</li>
</ol>
<p><strong>Kmeans的工作原理</strong>：</p>
<ul>
<li>基于距离度量（通常是欧氏距离）</li>
<li>将每个点分配到距离最近的簇中心</li>
<li>倾向于生成大小相近、形状规则（球形）的簇</li>
<li>必须将所有点分配到某个簇中</li>
</ul>
<p><strong>结果(b)为DBSCAN的特征</strong>：</p>
<ol>
<li><strong>簇的形状任意</strong>：三个簇的形状各异，能够识别出细长的、弯曲的簇结构</li>
<li><strong>簇的大小差异大</strong>：蓝色簇明显比其他两个簇大，DBSCAN不要求簇大小均衡</li>
<li><strong>能识别复杂形状</strong>：绿色簇呈现斜向的细长形状，橙色簇也是细长形状，这种非凸形状是DBSCAN的优势</li>
<li><strong>可能包含噪声点</strong>：DBSCAN可以将密度不足的点标记为噪声（虽然图中未明显显示）</li>
</ol>
<p><strong>DBSCAN的工作原理</strong>：</p>
<ul>
<li>基于<mark class="hl-label orange">密度</mark>的聚类方法</li>
<li>能够发现任意形状的簇</li>
<li>可以识别噪声点</li>
<li>不需要预先指定簇的数量</li>
</ul>
<p><strong>对比分析</strong>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>特征维度</th>
<th>结果(a) - Kmeans</th>
<th>结果(b) - DBSCAN</th>
</tr>
</thead>
<tbody>
<tr>
<td>簇的形状</td>
<td>规则、近似圆形</td>
<td>任意形状、细长弯曲</td>
</tr>
<tr>
<td>簇的大小</td>
<td>大小相近</td>
<td>大小差异明显</td>
</tr>
<tr>
<td>边界特点</td>
<td>直线或平滑曲线</td>
<td>沿着数据密度变化</td>
</tr>
<tr>
<td>复杂形状识别</td>
<td>差（只能识别凸形簇）</td>
<td>好（能识别任意形状）</td>
</tr>
</tbody>
</table>
</div>
<hr>
<p><strong>问题(2)：如果Kmeans和DBSCAN聚类效果接近，要从中选择聚类算法运算速度最快的，应该选哪一种，说明理由</strong></p>
<p><strong>答案</strong>：应该选择<strong>Kmeans</strong></p>
<p><strong>理由</strong>：</p>
<p><strong>1. 时间复杂度对比</strong></p>
<p><strong>Kmeans的时间复杂度</strong>：</p>
<ul>
<li>时间复杂度：$O(n \cdot k \cdot t)$<ul>
<li>$n$：样本数量</li>
<li>$k$：簇的数量</li>
<li>$t$：迭代次数（通常较小，如10-100次）</li>
</ul>
</li>
<li>每次迭代只需要：<ul>
<li>计算每个点到k个簇中心的距离：$O(n \cdot k)$</li>
<li>更新簇中心：$O(n)$</li>
</ul>
</li>
<li>迭代次数通常较少且可控</li>
</ul>
<p><strong>DBSCAN的时间复杂度</strong>：</p>
<ul>
<li>时间复杂度：$O(n^2)$（未优化时）</li>
<li>使用空间索引（如KD树、R树）优化后：$O(n \log n)$</li>
<li>需要对每个点：<ul>
<li>查找其邻域内的所有点</li>
<li>判断是否为核心点</li>
<li>扩展簇（可能需要递归访问多个点）</li>
</ul>
</li>
</ul>
<p><strong>2. 实际运算速度对比</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>对比维度</th>
<th>Kmeans</th>
<th>DBSCAN</th>
</tr>
</thead>
<tbody>
<tr>
<td>基本时间复杂度</td>
<td>$O(n \cdot k \cdot t)$</td>
<td>$O(n^2)$或$O(n \log n)$</td>
</tr>
<tr>
<td>大规模数据表现</td>
<td>快（线性复杂度）</td>
<td>慢（平方或对数复杂度）</td>
</tr>
<tr>
<td>迭代次数</td>
<td>可控（通常10-100次）</td>
<td>需要<mark class="hl-label orange">遍历所有点的邻域</mark></td>
</tr>
<tr>
<td>距离计算次数</td>
<td>$n \cdot k \cdot t$</td>
<td>可能需要$n^2$次</td>
</tr>
<tr>
<td>内存占用</td>
<td>小（只需存储k个簇中心）</td>
<td>大（需要存储邻域信息）</td>
</tr>
</tbody>
</table>
</div>
<p><strong>3. 具体分析</strong></p>
<p><strong>Kmeans更快的原因</strong>：</p>
<ol>
<li><p><strong>计算简单</strong>：</p>
<ul>
<li>每次迭代只需计算点到簇中心的距离</li>
<li>距离计算是简单的欧氏距离</li>
<li>更新簇中心只是求平均值</li>
</ul>
</li>
<li><p><strong>迭代次数少</strong>：</p>
<ul>
<li>通常10-100次迭代即可收敛</li>
<li>可以设置收敛条件提前终止</li>
</ul>
</li>
<li><p><strong>并行化容易</strong>：</p>
<ul>
<li>计算每个点到簇中心的距离是独立的</li>
<li>容易实现并行计算</li>
</ul>
</li>
</ol>
<p><strong>DBSCAN较慢的原因</strong>：</p>
<ol>
<li><p><strong>邻域查询复杂</strong>：</p>
<ul>
<li>需要为每个点查找半径$\epsilon$内的所有邻居</li>
<li>即使使用空间索引，也需要$O(n \log n)$的复杂度</li>
</ul>
</li>
<li><p><strong>簇扩展过程</strong>：</p>
<ul>
<li>发现核心点后需要递归扩展簇</li>
<li>可能需要多次访问同一个点</li>
</ul>
</li>
<li><p><strong>参数敏感</strong>：</p>
<ul>
<li>需要仔细选择$\epsilon$和MinPts参数</li>
<li>参数选择不当可能导致多次重新计算</li>
</ul>
</li>
</ol>
<p><strong>4. 数值示例</strong></p>
<p>假设有10,000个样本点：</p>
<p><strong>Kmeans</strong>（k=3，迭代50次）：</p>
<ul>
<li>计算量：$10,000 \times 3 \times 50 = 1,500,000$次距离计算</li>
</ul>
<p><strong>DBSCAN</strong>（未优化）：</p>
<ul>
<li>计算量：$10,000^2 / 2 = 50,000,000$次距离计算（最坏情况）</li>
</ul>
<p><strong>DBSCAN</strong>（使用KD树优化）：</p>
<ul>
<li>计算量：$10,000 \times \log_2(10,000) \approx 10,000 \times 13.3 = 133,000$次查询</li>
<li>但每次查询仍需要访问多个节点</li>
</ul>
<p><strong>结论</strong>：在聚类效果接近的情况下，Kmeans的运算速度明显快于DBSCAN，因此应该选择<strong>Kmeans</strong>。</p>
<hr>
<h4 id="Kmeans聚类算法"><a href="#Kmeans聚类算法" class="headerlink" title="Kmeans聚类算法"></a>Kmeans聚类算法</h4><p><strong>Kmeans算法简介</strong>：</p>
<p>Kmeans是一种基于距离的划分式聚类算法，通过迭代优化将数据划分为k个簇，使得簇内样本相似度高，簇间相似度低。</p>
<p><strong>算法步骤</strong>：</p>
<ol>
<li><strong>初始化</strong>：随机选择k个样本作为初始簇中心</li>
<li><strong>分配步骤</strong>：将每个样本分配到距离最近的簇中心</li>
<li><strong>更新步骤</strong>：重新计算每个簇的中心（簇内所有样本的均值）</li>
<li><strong>迭代</strong>：重复步骤2和3，直到簇中心不再变化或达到最大迭代次数</li>
</ol>
<p><strong>数学表示</strong>：</p>
<p><strong>目标函数</strong>（最小化簇内平方和）：</p>
<script type="math/tex; mode=display">J = \sum_{i=1}^{k} \sum_{x \in C_i} \lVert x - \mu_i \rVert^2</script><p>其中：</p>
<ul>
<li>$k$：簇的数量</li>
<li>$C_i$：第$i$个簇</li>
<li>$\mu_i$：第$i$个簇的中心</li>
<li>$\lVert x - \mu_i \rVert$：样本$x$到簇中心$\mu_i$的欧氏距离</li>
</ul>
<p><strong>簇中心更新公式</strong>：</p>
<script type="math/tex; mode=display">\mu_i = \frac{1}{|C_i|} \sum_{x \in C_i} x</script><p><strong>算法特点</strong>：</p>
<p><strong>优点</strong>：</p>
<ol>
<li><strong>简单易实现</strong>：算法原理直观，实现简单</li>
<li><strong>速度快</strong>：时间复杂度为$O(n \cdot k \cdot t)$，适合大规模数据</li>
<li><strong>可扩展性好</strong>：容易并行化，适合分布式计算</li>
<li><strong>收敛性好</strong>：通常能快速收敛到局部最优解</li>
</ol>
<p><strong>缺点</strong>：</p>
<ol>
<li><strong>需要预先指定k值</strong>：簇的数量需要事先确定</li>
<li><strong>对初始值敏感</strong>：不同的初始簇中心可能导致不同的结果</li>
<li><strong>只能识别凸形簇</strong>：对非凸形状的簇效果不好</li>
<li><strong>对离群点敏感</strong>：离群点会影响簇中心的计算</li>
<li><strong>假设簇大小相近</strong>：倾向于生成大小相近的簇</li>
</ol>
<p><strong>Kmeans算法示例</strong>：</p>
<p>假设有以下6个一维数据点：2, 4, 10, 12, 3, 20，要将它们聚类为2个簇（k=2）。</p>
<p><strong>初始化</strong>：随机选择2个点作为初始簇中心</p>
<ul>
<li>$\mu_1 = 2$</li>
<li>$\mu_2 = 4$</li>
</ul>
<p><strong>第1次迭代</strong>：</p>
<p><strong>分配步骤</strong>：</p>
<ul>
<li>点2：距离$\mu_1$=0，距离$\mu_2$=2，分配到簇1</li>
<li>点4：距离$\mu_1$=2，距离$\mu_2$=0，分配到簇2</li>
<li>点10：距离$\mu_1$=8，距离$\mu_2$=6，分配到簇2</li>
<li>点12：距离$\mu_1$=10，距离$\mu_2$=8，分配到簇2</li>
<li>点3：距离$\mu_1$=1，距离$\mu_2$=1，分配到簇1（相等时选第一个）</li>
<li>点20：距离$\mu_1$=18，距离$\mu_2$=16，分配到簇2</li>
</ul>
<p>结果：簇1={2, 3}，簇2={4, 10, 12, 20}</p>
<p><strong>更新步骤</strong>：</p>
<ul>
<li>$\mu_1 = (2+3)/2 = 2.5$</li>
<li>$\mu_2 = (4+10+12+20)/4 = 11.5$</li>
</ul>
<p><strong>第2次迭代</strong>：</p>
<p><strong>分配步骤</strong>：</p>
<ul>
<li>点2：距离$\mu_1$=0.5，距离$\mu_2$=9.5，分配到簇1</li>
<li>点4：距离$\mu_1$=1.5，距离$\mu_2$=7.5，分配到簇1</li>
<li>点10：距离$\mu_1$=7.5，距离$\mu_2$=1.5，分配到簇2</li>
<li>点12：距离$\mu_1$=9.5，距离$\mu_2$=0.5，分配到簇2</li>
<li>点3：距离$\mu_1$=0.5，距离$\mu_2$=8.5，分配到簇1</li>
<li>点20：距离$\mu_1$=17.5，距离$\mu_2$=8.5，分配到簇2</li>
</ul>
<p>结果：簇1={2, 3, 4}，簇2={10, 12, 20}</p>
<p><strong>更新步骤</strong>：</p>
<ul>
<li>$\mu_1 = (2+3+4)/3 = 3$</li>
<li>$\mu_2 = (10+12+20)/3 = 14$</li>
</ul>
<p><strong>第3次迭代</strong>：</p>
<p><strong>分配步骤</strong>：</p>
<ul>
<li>点2：距离$\mu_1$=1，距离$\mu_2$=12，分配到簇1</li>
<li>点4：距离$\mu_1$=1，距离$\mu_2$=10，分配到簇1</li>
<li>点10：距离$\mu_1$=7，距离$\mu_2$=4，分配到簇2</li>
<li>点12：距离$\mu_1$=9，距离$\mu_2$=2，分配到簇2</li>
<li>点3：距离$\mu_1$=0，距离$\mu_2$=11，分配到簇1</li>
<li>点20：距离$\mu_1$=17，距离$\mu_2$=6，分配到簇2</li>
</ul>
<p>结果：簇1={2, 3, 4}，簇2={10, 12, 20}</p>
<p>簇分配没有变化，算法收敛。</p>
<p><strong>最终结果</strong>：</p>
<ul>
<li>簇1={2, 3, 4}，中心=3</li>
<li>簇2={10, 12, 20}，中心=14</li>
</ul>
<hr>
<h4 id="DBSCAN聚类算法"><a href="#DBSCAN聚类算法" class="headerlink" title="DBSCAN聚类算法"></a>DBSCAN聚类算法</h4><p><strong>DBSCAN算法简介</strong>：</p>
<p>DBSCAN（Density-Based Spatial Clustering of Applications with Noise）是一种基于密度的聚类算法，能够发现任意形状的簇，并能识别噪声点。</p>
<p><strong>核心概念</strong>：</p>
<ol>
<li><strong>$\epsilon$-邻域</strong>：以点$p$为中心，半径为$\epsilon$的区域内的所有点</li>
<li><strong>核心点（Core Point）</strong>：$\epsilon$-邻域内至少包含MinPts个点的点</li>
<li><strong>边界点（Border Point）</strong>：不是核心点，但在某个核心点的$\epsilon$-邻域内的点</li>
<li><strong>噪声点（Noise Point）</strong>：既不是核心点也不是边界点的点</li>
</ol>
<p><strong>算法步骤</strong>：</p>
<ol>
<li><strong>标记核心点</strong>：遍历所有点，找出$\epsilon$-邻域内至少有MinPts个点的核心点</li>
<li><strong>连接核心点</strong>：如果两个核心点在彼此的$\epsilon$-邻域内，将它们连接到同一个簇</li>
<li><strong>分配边界点</strong>：将边界点分配到其所在的核心点的簇中</li>
<li><strong>标记噪声点</strong>：剩余的点标记为噪声</li>
</ol>
<p><strong>算法参数</strong>：</p>
<ul>
<li><strong>$\epsilon$（eps）</strong>：邻域半径，定义了”密集”的空间范围</li>
<li><strong>MinPts</strong>：成为核心点所需的最小邻居数量</li>
</ul>
<p><strong>算法特点</strong>：</p>
<p><strong>优点</strong>：</p>
<ol>
<li><strong>不需要预先指定簇数量</strong>：自动发现簇的数量</li>
<li><strong>能识别任意形状的簇</strong>：不限于凸形簇</li>
<li><strong>能识别噪声点</strong>：对离群点有鲁棒性</li>
<li><strong>对簇大小无要求</strong>：可以发现大小差异很大的簇</li>
</ol>
<p><strong>缺点</strong>：</p>
<ol>
<li><strong>对参数敏感</strong>：$\epsilon$和MinPts的选择对结果影响很大</li>
<li><strong>时间复杂度高</strong>：未优化时为$O(n^2)$</li>
<li><strong>对密度不均匀的数据效果差</strong>：难以处理密度差异大的簇</li>
<li><strong>高维数据效果差</strong>：在高维空间中，距离度量变得不可靠</li>
</ol>
<p><strong>DBSCAN算法示例</strong>：</p>
<p>假设有以下二维数据点，设置$\epsilon=1.5$，MinPts=3。</p>
<p>数据点：</p>
<ul>
<li>A(1,1), B(1,2), C(2,1), D(2,2)（密集区域1）</li>
<li>E(8,8), F(8,9), G(9,8), H(9,9)（密集区域2）</li>
<li>I(5,5)（孤立点）</li>
</ul>
<p><strong>步骤1：标记核心点</strong></p>
<p>计算每个点的$\epsilon$-邻域内的点数：</p>
<ul>
<li>点A：邻域内有B, C, D（距离都≤1.5），共4个点（包括自己），是核心点</li>
<li>点B：邻域内有A, C, D，共4个点，是核心点</li>
<li>点C：邻域内有A, B, D，共4个点，是核心点</li>
<li>点D：邻域内有A, B, C，共4个点，是核心点</li>
<li>点E：邻域内有F, G, H，共4个点，是核心点</li>
<li>点F：邻域内有E, G, H，共4个点，是核心点</li>
<li>点G：邻域内有E, F, H，共4个点，是核心点</li>
<li>点H：邻域内有E, F, G，共4个点，是核心点</li>
<li>点I：邻域内只有自己，共1个点，不是核心点</li>
</ul>
<p><strong>步骤2：连接核心点</strong></p>
<ul>
<li>A, B, C, D互相在彼此的邻域内，形成簇1</li>
<li>E, F, G, H互相在彼此的邻域内，形成簇2</li>
</ul>
<p><strong>步骤3：分配边界点</strong></p>
<ul>
<li>点I不在任何核心点的邻域内，标记为噪声点</li>
</ul>
<p><strong>最终结果</strong>：</p>
<ul>
<li>簇1：{A, B, C, D}</li>
<li>簇2：{E, F, G, H}</li>
<li>噪声：{I}</li>
</ul>
<hr>
<h4 id="Kmeans与DBSCAN对比总结"><a href="#Kmeans与DBSCAN对比总结" class="headerlink" title="Kmeans与DBSCAN对比总结"></a>Kmeans与DBSCAN对比总结</h4><div class="table-container">
<table>
<thead>
<tr>
<th>对比维度</th>
<th>Kmeans</th>
<th>DBSCAN</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>算法类型</strong></td>
<td>划分式聚类</td>
<td>密度聚类</td>
</tr>
<tr>
<td><strong>簇数量</strong></td>
<td>需要预先指定k</td>
<td>自动确定</td>
</tr>
<tr>
<td><strong>簇形状</strong></td>
<td>凸形（球形）</td>
<td>任意形状</td>
</tr>
<tr>
<td><strong>簇大小</strong></td>
<td>倾向于大小相近</td>
<td>可以差异很大</td>
</tr>
<tr>
<td><strong>噪声处理</strong></td>
<td>所有点都分配到簇</td>
<td>可以识别噪声点</td>
</tr>
<tr>
<td><strong>时间复杂度</strong></td>
<td>$O(n \cdot k \cdot t)$</td>
<td>$O(n^2)$或$O(n \log n)$</td>
</tr>
<tr>
<td><strong>空间复杂度</strong></td>
<td>$O(n+k)$</td>
<td>$O(n)$</td>
</tr>
<tr>
<td><strong>参数</strong></td>
<td>k（簇数量）</td>
<td>$\epsilon$（邻域半径）、MinPts（最小点数）</td>
</tr>
<tr>
<td><strong>初始化敏感性</strong></td>
<td>对初始簇中心敏感</td>
<td>对参数敏感</td>
</tr>
<tr>
<td><strong>适用场景</strong></td>
<td>簇形状规则、大小相近、数据量大</td>
<td>簇形状任意、有噪声、密度均匀</td>
</tr>
<tr>
<td><strong>优化难度</strong></td>
<td>容易并行化</td>
<td>较难并行化</td>
</tr>
<tr>
<td><strong>收敛性</strong></td>
<td>保证收敛到局部最优</td>
<td>一次遍历即可完成</td>
</tr>
</tbody>
</table>
</div>
<p><strong>选择建议</strong>：</p>
<ol>
<li><p><strong>选择Kmeans的情况</strong>：</p>
<ul>
<li>数据量大，需要快速聚类</li>
<li>簇的形状大致为球形</li>
<li>簇的大小相近</li>
<li>知道大致的簇数量</li>
</ul>
</li>
<li><p><strong>选择DBSCAN的情况</strong>：</p>
<ul>
<li>不知道簇的数量</li>
<li>簇的形状不规则（如细长形、环形）</li>
<li>数据中有噪声或离群点</li>
<li>簇的密度相对均匀</li>
</ul>
</li>
<li><p><strong>两者都不适合的情况</strong>：</p>
<ul>
<li>密度差异很大的数据：考虑OPTICS算法</li>
<li>高维数据：考虑降维后再聚类</li>
<li>层次结构数据：考虑层次聚类算法</li>
</ul>
</li>
</ol>
<hr>
<h4 id="邻近度矩阵"><a href="#邻近度矩阵" class="headerlink" title="邻近度矩阵"></a>邻近度矩阵</h4><p><img src="/2026/01/09/MachineCollectionFinalReview/44.png" alt="44"></p>
<p><strong>题目</strong>：已知点数据集的临近度矩阵如下图所示。</p>
<p><strong>邻近度矩阵</strong>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
<th>E</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>1.00</td>
<td>0.65</td>
<td>0.10</td>
<td>0.90</td>
<td>0.20</td>
</tr>
<tr>
<td>B</td>
<td>0.65</td>
<td>1.00</td>
<td>0.40</td>
<td>0.60</td>
<td>0.80</td>
</tr>
<tr>
<td>C</td>
<td>0.10</td>
<td>0.40</td>
<td>1.00</td>
<td>0.70</td>
<td>0.30</td>
</tr>
<tr>
<td>D</td>
<td>0.90</td>
<td>0.60</td>
<td>0.70</td>
<td>1.00</td>
<td>0.50</td>
</tr>
<tr>
<td>E</td>
<td>0.20</td>
<td>0.80</td>
<td>0.30</td>
<td>0.50</td>
<td>1.00</td>
</tr>
</tbody>
</table>
</div>
<p>(1) 图中的邻近度，是指数据之间的相似度还是相异度（距离）？为什么？</p>
<p>(2) 请写出基于最小距离（单链）和最大距离（全链）的层次凝聚聚类结果，树状图展示。</p>
<details class="toggle"><summary class="toggle-button" style>超绝无敌手写版本答案</summary><div class="toggle-content"><p><img src="/2026/01/09/MachineCollectionFinalReview/45.png" alt="45"></p>
<p><img src="/2026/01/09/MachineCollectionFinalReview/46.png" alt="46"></p>
</div></details>
<hr>
<p><strong>解答过程</strong>：</p>
<p><strong>问题(1)：图中的邻近度，是指数据之间的相似度还是相异度（距离）？为什么？</strong></p>
<p><strong>答案</strong>：图中的邻近度是指<strong>相似度</strong></p>
<p><strong>理由</strong>：</p>
<ol>
<li><p><strong>对角线元素为1.00</strong>：</p>
<ul>
<li>矩阵的对角线元素表示每个点与自己的邻近度</li>
<li>对角线元素都是1.00，表示每个点与自己完全相同</li>
<li>如果是距离（相异度），对角线应该是0（点到自己的距离为0）</li>
<li>如果是相似度，对角线应该是1（点与自己完全相似）</li>
<li>因此，这是相似度矩阵</li>
</ul>
</li>
<li><p><strong>数值范围在[0, 1]之间</strong>：</p>
<ul>
<li>所有非对角线元素都在0到1之间</li>
<li>相似度通常归一化到[0, 1]区间，1表示完全相似，0表示完全不相似</li>
<li>距离通常是非负实数，没有上界限制</li>
</ul>
</li>
<li><p><strong>矩阵对称性</strong>：</p>
<ul>
<li>矩阵是对称的（如A-B = 0.65，B-A = 0.65）</li>
<li>无论是相似度还是距离，邻近度矩阵都应该是对称的</li>
<li>这一点不能单独判断，但与前两点结合可以确认</li>
</ul>
</li>
</ol>
<p><strong>相似度与距离的关系</strong>：</p>
<p>如果已知相似度矩阵，可以转换为距离矩阵：</p>
<script type="math/tex; mode=display">d(i,j) = 1 - s(i,j)</script><p>或者：</p>
<script type="math/tex; mode=display">d(i,j) = \sqrt{1 - s(i,j)}</script><p>其中$s(i,j)$是相似度，$d(i,j)$是距离。</p>
<p><strong>本题的距离矩阵</strong>（使用$d = 1 - s$转换）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
<th>E</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>0.00</td>
<td>0.35</td>
<td>0.90</td>
<td>0.10</td>
<td>0.80</td>
</tr>
<tr>
<td>B</td>
<td>0.35</td>
<td>0.00</td>
<td>0.60</td>
<td>0.40</td>
<td>0.20</td>
</tr>
<tr>
<td>C</td>
<td>0.90</td>
<td>0.60</td>
<td>0.00</td>
<td>0.30</td>
<td>0.70</td>
</tr>
<tr>
<td>D</td>
<td>0.10</td>
<td>0.40</td>
<td>0.30</td>
<td>0.00</td>
<td>0.50</td>
</tr>
<tr>
<td>E</td>
<td>0.80</td>
<td>0.20</td>
<td>0.70</td>
<td>0.50</td>
<td>0.00</td>
</tr>
</tbody>
</table>
</div>
<hr>
<p><strong>问题(2)：请写出基于最小距离（单链）和最大距离（全链）的层次凝聚聚类结果，树状图展示</strong></p>
<p>首先，我们使用转换后的距离矩阵进行层次聚类。</p>
<p><strong>距离矩阵</strong>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
<th>E</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>0.00</td>
<td>0.35</td>
<td>0.90</td>
<td>0.10</td>
<td>0.80</td>
</tr>
<tr>
<td>B</td>
<td>0.35</td>
<td>0.00</td>
<td>0.60</td>
<td>0.40</td>
<td>0.20</td>
</tr>
<tr>
<td>C</td>
<td>0.90</td>
<td>0.60</td>
<td>0.00</td>
<td>0.30</td>
<td>0.70</td>
</tr>
<tr>
<td>D</td>
<td>0.10</td>
<td>0.40</td>
<td>0.30</td>
<td>0.00</td>
<td>0.50</td>
</tr>
<tr>
<td>E</td>
<td>0.80</td>
<td>0.20</td>
<td>0.70</td>
<td>0.50</td>
<td>0.00</td>
</tr>
</tbody>
</table>
</div>
<hr>
<p><strong>方法1：单链（Single Linkage，最小距离）</strong></p>
<p><strong>单链定义</strong>：两个簇之间的距离定义为两个簇中最近的两个点之间的距离。</p>
<script type="math/tex; mode=display">d_{min}(C_i, C_j) = \min_{x \in C_i, y \in C_j} d(x, y)</script><p><strong>聚类过程</strong>：</p>
<p><strong>初始状态</strong>：每个点是一个簇</p>
<ul>
<li>簇：{A}, {B}, {C}, {D}, {E}</li>
</ul>
<p><strong>第1步</strong>：找到最小距离</p>
<ul>
<li>最小距离：d(A, D) = 0.10</li>
<li>合并：{A, D}</li>
<li>当前簇：{A, D}, {B}, {C}, {E}</li>
</ul>
<p><strong>更新距离矩阵</strong>：<br>计算{A,D}与其他簇的距离（取最小值）：</p>
<ul>
<li>d({A,D}, B) = min(d(A,B), d(D,B)) = min(0.35, 0.40) = 0.35</li>
<li>d({A,D}, C) = min(d(A,C), d(D,C)) = min(0.90, 0.30) = 0.30</li>
<li>d({A,D}, E) = min(d(A,E), d(D,E)) = min(0.80, 0.50) = 0.50</li>
</ul>
<p>新距离矩阵：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>{A,D}</th>
<th>B</th>
<th>C</th>
<th>E</th>
</tr>
</thead>
<tbody>
<tr>
<td>{A,D}</td>
<td>0.00</td>
<td>0.35</td>
<td>0.30</td>
<td>0.50</td>
</tr>
<tr>
<td>B</td>
<td>0.35</td>
<td>0.00</td>
<td>0.60</td>
<td>0.20</td>
</tr>
<tr>
<td>C</td>
<td>0.30</td>
<td>0.60</td>
<td>0.00</td>
<td>0.70</td>
</tr>
<tr>
<td>E</td>
<td>0.50</td>
<td>0.20</td>
<td>0.70</td>
<td>0.00</td>
</tr>
</tbody>
</table>
</div>
<p><strong>第2步</strong>：找到最小距离</p>
<ul>
<li>最小距离：d(B, E) = 0.20</li>
<li>合并：{B, E}</li>
<li>当前簇：{A, D}, {B, E}, {C}</li>
</ul>
<p><strong>更新距离矩阵</strong>：<br>计算{B,E}与其他簇的距离：</p>
<ul>
<li>d({A,D}, {B,E}) = min(d({A,D},B), d({A,D},E)) = min(0.35, 0.50) = 0.35</li>
<li>d({B,E}, C) = min(d(B,C), d(E,C)) = min(0.60, 0.70) = 0.60</li>
</ul>
<p>新距离矩阵：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>{A,D}</th>
<th>{B,E}</th>
<th>C</th>
</tr>
</thead>
<tbody>
<tr>
<td>{A,D}</td>
<td>0.00</td>
<td>0.35</td>
<td>0.30</td>
</tr>
<tr>
<td>{B,E}</td>
<td>0.35</td>
<td>0.00</td>
<td>0.60</td>
</tr>
<tr>
<td>C</td>
<td>0.30</td>
<td>0.60</td>
<td>0.00</td>
</tr>
</tbody>
</table>
</div>
<p><strong>第3步</strong>：找到最小距离</p>
<ul>
<li>最小距离：d({A,D}, C) = 0.30</li>
<li>合并：{A, D, C}</li>
<li>当前簇：{A, D, C}, {B, E}</li>
</ul>
<p><strong>更新距离矩阵</strong>：<br>计算{A,D,C}与{B,E}的距离：</p>
<ul>
<li>d({A,D,C}, {B,E}) = min(d({A,D},{B,E}), d(C,{B,E})) = min(0.35, 0.60) = 0.35</li>
</ul>
<p>新距离矩阵：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>{A,D,C}</th>
<th>{B,E}</th>
</tr>
</thead>
<tbody>
<tr>
<td>{A,D,C}</td>
<td>0.00</td>
<td>0.35</td>
</tr>
<tr>
<td>{B,E}</td>
<td>0.35</td>
<td>0.00</td>
</tr>
</tbody>
</table>
</div>
<p><strong>第4步</strong>：找到最小距离</p>
<ul>
<li>最小距离：d({A,D,C}, {B,E}) = 0.35</li>
<li>合并：{A, D, C, B, E}</li>
<li>当前簇：{A, D, C, B, E}</li>
</ul>
<p><strong>聚类完成</strong></p>
<p><strong>单链聚类树状图</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">       ┌─────────────────────┐</span><br><span class="line">       │                     │ 0.35</span><br><span class="line">  ┌────┴────┐           ┌────┴────┐</span><br><span class="line">  │         │ 0.30      │         │ 0.20</span><br><span class="line">┌─┴─┐       C         ┌─┴─┐       E</span><br><span class="line">│   │ 0.10            B   </span><br><span class="line">A   D                 </span><br></pre></td></tr></table></figure>
<p>或者用文字表示：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">合并顺序：</span><br><span class="line">1. A-D (距离0.10)</span><br><span class="line">2. B-E (距离0.20)</span><br><span class="line">3. &#123;A,D&#125;-C (距离0.30)</span><br><span class="line">4. &#123;A,D,C&#125;-&#123;B,E&#125; (距离0.35)</span><br></pre></td></tr></table></figure></p>
<hr>
<p><strong>方法2：全链（Complete Linkage，最大距离）</strong></p>
<p><strong>全链定义</strong>：两个簇之间的距离定义为两个簇中最远的两个点之间的距离。</p>
<script type="math/tex; mode=display">d_{max}(C_i, C_j) = \max_{x \in C_i, y \in C_j} d(x, y)</script><p><strong>聚类过程</strong>：</p>
<p><strong>初始状态</strong>：每个点是一个簇</p>
<ul>
<li>簇：{A}, {B}, {C}, {D}, {E}</li>
</ul>
<p><strong>第1步</strong>：找到最小距离</p>
<ul>
<li>最小距离：d(A, D) = 0.10</li>
<li>合并：{A, D}</li>
<li>当前簇：{A, D}, {B}, {C}, {E}</li>
</ul>
<p><strong>更新距离矩阵</strong>：<br>计算{A,D}与其他簇的距离（取最大值）：</p>
<ul>
<li>d({A,D}, B) = max(d(A,B), d(D,B)) = max(0.35, 0.40) = 0.40</li>
<li>d({A,D}, C) = max(d(A,C), d(D,C)) = max(0.90, 0.30) = 0.90</li>
<li>d({A,D}, E) = max(d(A,E), d(D,E)) = max(0.80, 0.50) = 0.80</li>
</ul>
<p>新距离矩阵：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>{A,D}</th>
<th>B</th>
<th>C</th>
<th>E</th>
</tr>
</thead>
<tbody>
<tr>
<td>{A,D}</td>
<td>0.00</td>
<td>0.40</td>
<td>0.90</td>
<td>0.80</td>
</tr>
<tr>
<td>B</td>
<td>0.40</td>
<td>0.00</td>
<td>0.60</td>
<td>0.20</td>
</tr>
<tr>
<td>C</td>
<td>0.90</td>
<td>0.60</td>
<td>0.00</td>
<td>0.70</td>
</tr>
<tr>
<td>E</td>
<td>0.80</td>
<td>0.20</td>
<td>0.70</td>
<td>0.00</td>
</tr>
</tbody>
</table>
</div>
<p><strong>第2步</strong>：找到最小距离</p>
<ul>
<li>最小距离：d(B, E) = 0.20</li>
<li>合并：{B, E}</li>
<li>当前簇：{A, D}, {B, E}, {C}</li>
</ul>
<p><strong>更新距离矩阵</strong>：<br>计算{B,E}与其他簇的距离：</p>
<ul>
<li>d({A,D}, {B,E}) = max(d(A,B), d(A,E), d(D,B), d(D,E)) = max(0.35, 0.80, 0.40, 0.50) = 0.80</li>
<li>d({B,E}, C) = max(d(B,C), d(E,C)) = max(0.60, 0.70) = 0.70</li>
</ul>
<p>新距离矩阵：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>{A,D}</th>
<th>{B,E}</th>
<th>C</th>
</tr>
</thead>
<tbody>
<tr>
<td>{A,D}</td>
<td>0.00</td>
<td>0.80</td>
<td>0.90</td>
</tr>
<tr>
<td>{B,E}</td>
<td>0.80</td>
<td>0.00</td>
<td>0.70</td>
</tr>
<tr>
<td>C</td>
<td>0.90</td>
<td>0.70</td>
<td>0.00</td>
</tr>
</tbody>
</table>
</div>
<p><strong>第3步</strong>：找到最小距离</p>
<ul>
<li>最小距离：d({B,E}, C) = 0.70</li>
<li>合并：{B, E, C}</li>
<li>当前簇：{A, D}, {B, E, C}</li>
</ul>
<p><strong>更新距离矩阵</strong>：<br>计算{A,D}与{B,E,C}的距离：</p>
<ul>
<li>d({A,D}, {B,E,C}) = max(d(A,B), d(A,E), d(A,C), d(D,B), d(D,E), d(D,C))</li>
<li>= max(0.35, 0.80, 0.90, 0.40, 0.50, 0.30) = 0.90</li>
</ul>
<p>新距离矩阵：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>{A,D}</th>
<th>{B,E,C}</th>
</tr>
</thead>
<tbody>
<tr>
<td>{A,D}</td>
<td>0.00</td>
<td>0.90</td>
</tr>
<tr>
<td>{B,E,C}</td>
<td>0.90</td>
<td>0.00</td>
</tr>
</tbody>
</table>
</div>
<p><strong>第4步</strong>：找到最小距离</p>
<ul>
<li>最小距离：d({A,D}, {B,E,C}) = 0.90</li>
<li>合并：{A, D, B, E, C}</li>
<li>当前簇：{A, D, B, E, C}</li>
</ul>
<p><strong>聚类完成</strong></p>
<p><strong>全链聚类树状图</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">     ┌──────────────────┐</span><br><span class="line">     │                  │ 0.90</span><br><span class="line">┌────┴────┐        ┌────┴────┐</span><br><span class="line">│         │ 0.10   │         │ 0.70</span><br><span class="line">A         D      ┌─┴─┐       C</span><br><span class="line">                 │   │ 0.20</span><br><span class="line">                 B   E</span><br></pre></td></tr></table></figure>
<p>或者用文字表示：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">合并顺序：</span><br><span class="line">1. A-D (距离0.10)</span><br><span class="line">2. B-E (距离0.20)</span><br><span class="line">3. &#123;B,E&#125;-C (距离0.70)</span><br><span class="line">4. &#123;A,D&#125;-&#123;B,E,C&#125; (距离0.90)</span><br></pre></td></tr></table></figure></p>
<hr>
<p><strong>单链与全链对比</strong>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>对比维度</th>
<th>单链（最小距离）</th>
<th>全链（最大距离）</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>合并顺序</strong></td>
<td>1. A-D (0.10)<br>2. B-E (0.20)<br>3. {A,D}-C (0.30)<br>4. {A,D,C}-{B,E} (0.35)</td>
<td>1. A-D (0.10)<br>2. B-E (0.20)<br>3. {B,E}-C (0.70)<br>4. {A,D}-{B,E,C} (0.90)</td>
</tr>
<tr>
<td><strong>第3步差异</strong></td>
<td>C先与{A,D}合并</td>
<td>C先与{B,E}合并</td>
</tr>
<tr>
<td><strong>最终合并距离</strong></td>
<td>0.35</td>
<td>0.90</td>
</tr>
<tr>
<td><strong>簇的紧密度</strong></td>
<td>较松散</td>
<td>较紧密</td>
</tr>
</tbody>
</table>
</div>
<p><strong>关键差异分析</strong>：</p>
<ol>
<li><p><strong>第3步的不同选择</strong>：</p>
<ul>
<li>单链：d({A,D}, C) = 0.30 &lt; d({B,E}, C) = 0.60，所以C与{A,D}合并</li>
<li>全链：d({B,E}, C) = 0.70 &lt; d({A,D}, C) = 0.90，所以C与{B,E}合并</li>
</ul>
</li>
<li><p><strong>最终合并距离的差异</strong>：</p>
<ul>
<li>单链：0.35（两个簇中最近的点之间的距离）</li>
<li>全链：0.90（两个簇中最远的点之间的距离）</li>
</ul>
</li>
<li><p><strong>簇的形状特点</strong>：</p>
<ul>
<li>单链：倾向于产生链状、细长的簇，对噪声敏感</li>
<li>全链：倾向于产生紧凑、球形的簇，对噪声不敏感</li>
</ul>
</li>
</ol>
<hr>
<h4 id="层次聚类算法"><a href="#层次聚类算法" class="headerlink" title="层次聚类算法"></a>层次聚类算法</h4><p><strong>层次聚类简介</strong>：</p>
<p>层次聚类（Hierarchical Clustering）是一种通过计算不同类别数据点之间的相似度来创建聚类树的聚类方法。主要分为两类：</p>
<ol>
<li><strong>凝聚层次聚类（Agglomerative）</strong>：自底向上，从单个点开始，逐步合并</li>
<li><strong>分裂层次聚类（Divisive）</strong>：自顶向下，从所有点开始，逐步分裂</li>
</ol>
<p><strong>凝聚层次聚类算法步骤</strong>：</p>
<ol>
<li><strong>初始化</strong>：将每个数据点视为一个簇</li>
<li><strong>计算距离</strong>：计算所有簇之间的距离</li>
<li><strong>合并簇</strong>：找到距离最小的两个簇，将它们合并</li>
<li><strong>更新距离</strong>：重新计算新簇与其他簇的距离</li>
<li><strong>重复</strong>：重复步骤3-4，直到所有点合并为一个簇</li>
</ol>
<p><strong>簇间距离的定义方法</strong>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>定义</th>
<th>公式</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>单链（Single Linkage）</strong></td>
<td>两个簇中最近的两个点之间的距离</td>
<td>$d<em>{min}(C_i, C_j) = \min</em>{x \in C_i, y \in C_j} d(x, y)$</td>
<td>容易产生链状簇，对噪声敏感</td>
</tr>
<tr>
<td><strong>全链（Complete Linkage）</strong></td>
<td>两个簇中最远的两个点之间的距离</td>
<td>$d<em>{max}(C_i, C_j) = \max</em>{x \in C_i, y \in C_j} d(x, y)$</td>
<td>产生紧凑簇，对噪声不敏感</td>
</tr>
<tr>
<td><strong>平均链（Average Linkage）</strong></td>
<td>两个簇中所有点对之间距离的平均值</td>
<td>$d<em>{avg}(C_i, C_j) = \frac{1}{&#124;C_i&#124; \cdot &#124;C_j&#124;} \sum</em>{x \in C_i, y \in C_j} d(x, y)$</td>
<td>折中方案，较为稳定</td>
</tr>
<tr>
<td><strong>质心法（Centroid）</strong></td>
<td>两个簇的质心之间的距离</td>
<td>$d_{centroid}(C_i, C_j) = d(\mu_i, \mu_j)$</td>
<td>受离群点影响较大</td>
</tr>
<tr>
<td><strong>Ward法</strong></td>
<td>合并后簇内平方和的增量</td>
<td>$\Delta(C<em>i, C_j) = \sum</em>{x \in C<em>i \cup C_j} &#124;&#124;x - \mu</em>{ij}&#124;&#124;^2 - \sum<em>{x \in C_i} &#124;&#124;x - \mu_i&#124;&#124;^2 - \sum</em>{x \in C_j} &#124;&#124;x - \mu_j&#124;&#124;^2$</td>
<td>倾向于产生大小相近的簇</td>
</tr>
</tbody>
</table>
</div>
<p><strong>层次聚类的优缺点</strong>：</p>
<p><strong>优点</strong>：</p>
<ol>
<li><strong>不需要预先指定簇数量</strong>：可以通过树状图选择合适的簇数量</li>
<li><strong>结果直观</strong>：树状图清晰展示了数据的层次结构</li>
<li><strong>适用于任意形状的簇</strong>：不同的距离定义适用于不同形状</li>
<li><strong>确定性</strong>：对于相同的数据和参数，结果是确定的</li>
</ol>
<p><strong>缺点</strong>：</p>
<ol>
<li><strong>时间复杂度高</strong>：$O(n^2 \log n)$或$O(n^3)$，不适合大规模数据</li>
<li><strong>空间复杂度高</strong>：需要存储距离矩阵，$O(n^2)$</li>
<li><strong>不可撤销</strong>：一旦合并就无法撤销，错误会传播</li>
<li><strong>对噪声和离群点敏感</strong>：特别是单链方法</li>
</ol>
<p><strong>树状图（Dendrogram）</strong>：</p>
<p>树状图是层次聚类结果的可视化表示，纵轴表示簇之间的距离，横轴表示数据点。</p>
<p><strong>如何从树状图确定簇数量</strong>：</p>
<ol>
<li><strong>水平切割</strong>：在树状图的某个高度画一条水平线</li>
<li><strong>簇数量</strong>：水平线穿过的垂直线段数量即为簇的数量</li>
<li><strong>选择标准</strong>：<ul>
<li>选择距离增量最大的位置切割</li>
<li>根据业务需求选择合适的簇数量</li>
<li>使用肘部法则（Elbow Method）</li>
</ul>
</li>
</ol>
<p><strong>示例</strong>：</p>
<p>对于本题的单链聚类结果，如果在距离0.30处切割：</p>
<ul>
<li>得到3个簇：{A, D}, {C}, {B, E}</li>
</ul>
<p>如果在距离0.20处切割：</p>
<ul>
<li>得到4个簇：{A, D}, {B, E}, {C}, 以及可能的其他点</li>
</ul>
<hr>
<h4 id="相似度与距离度量"><a href="#相似度与距离度量" class="headerlink" title="相似度与距离度量"></a>相似度与距离度量</h4><p><strong>相似度（Similarity）</strong>：</p>
<p>相似度衡量两个对象的相似程度，值越大表示越相似。</p>
<p><strong>常用相似度度量</strong>：</p>
<ol>
<li><p><strong>余弦相似度（Cosine Similarity）</strong>：</p>
<script type="math/tex; mode=display">\text{sim}(x, y) = \frac{x \cdot y}{||x|| \cdot ||y||} = \frac{\sum_{i=1}^{n} x_i y_i}{\sqrt{\sum_{i=1}^{n} x_i^2} \cdot \sqrt{\sum_{i=1}^{n} y_i^2}}</script><ul>
<li>取值范围：[-1, 1]</li>
<li>常用于文本相似度计算</li>
</ul>
</li>
<li><p><strong>Jaccard相似度</strong>：</p>
<script type="math/tex; mode=display">\text{sim}(A, B) = \frac{|A \cap B|}{|A \cup B|}</script><ul>
<li>取值范围：[0, 1]</li>
<li>常用于集合相似度计算</li>
</ul>
</li>
<li><p><strong>皮尔逊相关系数（Pearson Correlation）</strong>：</p>
<script type="math/tex; mode=display">\text{sim}(x, y) = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2} \cdot \sqrt{\sum_{i=1}^{n} (y_i - \bar{y})^2}}</script><ul>
<li>取值范围：[-1, 1]</li>
<li>衡量线性相关性</li>
</ul>
</li>
</ol>
<p><strong>距离（Distance/Dissimilarity）</strong>：</p>
<p>距离衡量两个对象的差异程度，值越大表示越不相似。</p>
<p><strong>常用距离度量</strong>：</p>
<ol>
<li><p><strong>欧氏距离（Euclidean Distance）</strong>：</p>
<script type="math/tex; mode=display">d(x, y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}</script><ul>
<li>最常用的距离度量</li>
<li>对量纲敏感，需要归一化</li>
</ul>
</li>
<li><p><strong>曼哈顿距离（Manhattan Distance）</strong>：</p>
<script type="math/tex; mode=display">d(x, y) = \sum_{i=1}^{n} |x_i - y_i|</script><ul>
<li>也称为城市街区距离</li>
<li>对离群点不敏感</li>
</ul>
</li>
<li><p><strong>闵可夫斯基距离（Minkowski Distance）</strong>：</p>
<script type="math/tex; mode=display">d(x, y) = \left(\sum_{i=1}^{n} |x_i - y_i|^p\right)^{1/p}</script><ul>
<li>p=1：曼哈顿距离</li>
<li>p=2：欧氏距离</li>
<li>p→∞：切比雪夫距离</li>
</ul>
</li>
<li><p><strong>切比雪夫距离（Chebyshev Distance）</strong>：</p>
<script type="math/tex; mode=display">d(x, y) = \max_{i=1}^{n} |x_i - y_i|</script><ul>
<li>取各维度差异的最大值</li>
</ul>
</li>
</ol>
<p><strong>相似度与距离的转换</strong>：</p>
<ol>
<li><p><strong>线性转换</strong>：</p>
<script type="math/tex; mode=display">d = 1 - s \quad \text{或} \quad s = 1 - d</script><ul>
<li>适用于相似度和距离都在[0, 1]范围内</li>
</ul>
</li>
<li><p><strong>倒数转换</strong>：</p>
<script type="math/tex; mode=display">d = \frac{1}{s} \quad \text{或} \quad s = \frac{1}{1 + d}</script><ul>
<li>适用于相似度为正值的情况</li>
</ul>
</li>
<li><p><strong>指数转换</strong>：</p>
<script type="math/tex; mode=display">s = e^{-d} \quad \text{或} \quad d = -\ln(s)</script><ul>
<li>适用于需要非线性转换的情况</li>
</ul>
</li>
</ol>
<p><strong>选择距离度量的考虑因素</strong>：</p>
<ol>
<li><p><strong>数据类型</strong>：</p>
<ul>
<li>数值型：欧氏距离、曼哈顿距离</li>
<li>二值型：Jaccard距离、汉明距离</li>
<li>文本型：余弦相似度、编辑距离</li>
</ul>
</li>
<li><p><strong>数据分布</strong>：</p>
<ul>
<li>正态分布：欧氏距离</li>
<li>有离群点：曼哈顿距离</li>
<li>高维数据：余弦相似度</li>
</ul>
</li>
<li><p><strong>量纲问题</strong>：</p>
<ul>
<li>不同量纲的特征需要归一化</li>
<li>或使用对量纲不敏感的度量（如余弦相似度）</li>
</ul>
</li>
<li><p><strong>计算效率</strong>：</p>
<ul>
<li>欧氏距离计算较快</li>
<li>编辑距离计算较慢</li>
</ul>
</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://github.com/XBXyftx">XBXyftx</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://xbxyftx.top">https://xbxyftx.top</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">此文章版权归XBXyftx所有，如有转载，请註明来自原作者</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/">期末复习</a><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/">机器学习与数据挖掘</a></div><div class="post-share"><div class="social-share" data-image="/imgs/ArticleTopImgs/MachineCollectionFinalReviewTopImg.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://lib.baomitu.com/social-share.js/1.0.16/css/share.min.css" media="print" onload="this.media='all'"><script src="https://lib.baomitu.com/social-share.js/1.0.16/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2026/01/06/osFinalReview/" title="操作系统期末复习"><img class="cover" src="/imgs/ArticleTopImgs/osFinalReviewTopImg.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">操作系统期末复习</div></div><div class="info-2"><div class="info-item-1">计算机网络期末复习</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2026/01/06/osFinalReview/" title="操作系统期末复习"><img class="cover" src="/imgs/ArticleTopImgs/osFinalReviewTopImg.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-06</div><div class="info-item-2">操作系统期末复习</div></div><div class="info-2"><div class="info-item-1">计算机网络期末复习</div></div></div></a><a class="pagination-related" href="/2026/01/04/computerNetFinalReview/" title="计算机网络期末复习"><img class="cover" src="/imgs/ArticleTopImgs/computerNetFinalReviewTopImg.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-04</div><div class="info-item-2">计算机网络期末复习</div></div><div class="info-2"><div class="info-item-1">计算机网络期末复习</div></div></div></a><a class="pagination-related" href="/2025/12/24/dataCollectionFinalReview/" title="数据采集实践期末复习"><img class="cover" src="/imgs/ArticleTopImgs/dataCollectionFinalReviewTopImg.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-24</div><div class="info-item-2">数据采集实践期末复习</div></div><div class="info-2"><div class="info-item-1">数据采集期末复习</div></div></div></a><a class="pagination-related" href="/2025/05/18/SAS/" title="《数据分析方法（第二版）》重要概念"><img class="cover" src="/imgs/ArticleTopImgs/SASTopImg.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-18</div><div class="info-item-2">《数据分析方法（第二版）》重要概念</div></div><div class="info-2"><div class="info-item-1">统分期末复习</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/logo.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">XBXyftx</div><div class="author-info-description">博安千古情如此<br/>璇玉如华自醉从</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">45</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">44</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/XBXyftx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/XBXyftx" target="_blank" title="Github"><i class="fab fa-github" style="color: #c3ffff;"></i></a><a class="social-icon" href="mailto:shuaixbx02@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="/img/wechat.png" target="_blank" title="WeChat"><i class="fa-brands fa-weixin" style="color: #7bb342;"></i></a></div></div><div class="card-widget card-info visitor-info"><div class="card-content"><div class="visitor-welcome"><i class="fas fa-heart"></i><span id="visitor-text">欢迎来访！</span></div><div class="visitor-details"><div class="visitor-location"><i class="fas fa-map-marker-alt"></i><span id="visitor-location-text">正在获取位置信息...</span></div><div class="visitor-ip"><i class="fas fa-globe"></i><span>IP地址: </span><span id="visitor-ip-text">获取中...</span></div><div class="visitor-distance"><i class="fas fa-route"></i><span id="visitor-distance-text">距离博主 加载中...</span></div><div class="visitor-tip"><i class="fas fa-moon"></i><span id="visitor-tip-text">晚上好，夜生活嗨起来！</span></div></div></div><script>// 访问者信息获取
(function() {
  // IP地址获取API列表（按优先级排序）
  const ipApis = [
    'https://api.ip.sb/geoip',
    'https://ipapi.co/json/',
    'https://freegeoip.app/json/',
    'https://ipinfo.io/json'
  ];
  
  let currentApiIndex = 0;
  
  function tryGetLocation() {
    if (currentApiIndex >= ipApis.length) {
      // 所有API都失败了，显示错误信息
      handleError('所有IP服务都无法访问');
      return;
    }
    
    const currentApi = ipApis[currentApiIndex];
    
    fetch(currentApi)
      .then(response => {
        if (!response.ok) throw new Error(`HTTP ${response.status}`);
        return response.json();
      })
      .then(data => {
        processLocationData(data, currentApi);
      })
      .catch(error => {
        console.warn(`API ${currentApi} 失败:`, error);
        currentApiIndex++;
        setTimeout(tryGetLocation, 1000); // 1秒后尝试下一个API
      });
  }
  
  function processLocationData(data, apiUrl) {
    let country, region, city, ip, latitude, longitude;
    
    // 根据不同API格式处理数据
    if (apiUrl.includes('ip.sb')) {
      country = data.country || '未知';
      region = data.region || '未知';
      city = data.city || '未知';
      ip = data.ip || '未知';
      latitude = data.latitude;
      longitude = data.longitude;
    } else if (apiUrl.includes('ipapi.co')) {
      country = data.country_name || '未知';
      region = data.region || '未知';
      city = data.city || '未知';
      ip = data.ip || '未知';
      latitude = data.latitude;
      longitude = data.longitude;
    } else if (apiUrl.includes('freegeoip.app')) {
      country = data.country_name || '未知';
      region = data.region_name || '未知';
      city = data.city || '未知';
      ip = data.ip || '未知';
      latitude = data.latitude;
      longitude = data.longitude;
    } else if (apiUrl.includes('ipinfo.io')) {
      country = data.country || '未知';
      const loc = data.region || '未知';
      region = loc;
      city = data.city || '未知';
      ip = data.ip || '未知';
      if (data.loc) {
        const coords = data.loc.split(',');
        latitude = parseFloat(coords[0]);
        longitude = parseFloat(coords[1]);
      }
    }
    
    // 更新IP地址
    document.getElementById('visitor-ip-text').textContent = ip;
    
    // 更新位置信息和欢迎文字
    let locationText = '';
    if (country === '中国' || country === 'China' || country === 'CN') {
      locationText = `${region} ${city}`;
                    document.getElementById('visitor-text').innerHTML = `欢迎来自 ${region} ${city} 的朋友♥️`;
    } else {
      locationText = `${country} ${region}`;
      document.getElementById('visitor-text').innerHTML = `欢迎来自 ${country} 的朋友♥️`;
    }
    document.getElementById('visitor-location-text').textContent = locationText;
    
    // 计算距离（可配置博主位置）
    if (latitude && longitude && !isNaN(latitude) && !isNaN(longitude)) {
      const bloggerLat = 39.9042; // 北京纬度
      const bloggerLng = 116.4074; // 北京经度
      const distance = calculateDistance(latitude, longitude, bloggerLat, bloggerLng);
      
      if (distance < 1) {
        document.getElementById('visitor-distance-text').textContent = `距离博主 ${(distance * 1000).toFixed(0)} 米！`;
      } else if (distance < 100) {
        document.getElementById('visitor-distance-text').textContent = `距离博主 ${distance.toFixed(1)} 公里！`;
      } else {
        document.getElementById('visitor-distance-text').textContent = `距离博主 ${distance.toFixed(0)} 公里！`;
      }
    } else {
      document.getElementById('visitor-distance-text').textContent = '距离计算中...';
    }
    
    // 根据时间设置问候语
    updateGreeting();
  }
  
  function updateGreeting() {
    const now = new Date();
    const hour = now.getHours();
    let greeting = '';
    let icon = 'fas fa-sun';
    
    if (hour >= 6 && hour < 12) {
      greeting = '上午好，新的一天开始了！';
      icon = 'fas fa-sun';
    } else if (hour >= 12 && hour < 14) {
      greeting = '中午好，记得吃午饭哦！';
      icon = 'fas fa-sun';
    } else if (hour >= 14 && hour < 18) {
      greeting = '下午好，午后时光真美好！';
      icon = 'fas fa-cloud-sun';
    } else if (hour >= 18 && hour < 22) {
      greeting = '晚上好，夜生活嗨起来！';
      icon = 'fas fa-moon';
    } else {
      greeting = '深夜了，注意休息哦！';
      icon = 'fas fa-moon';
    }
    
    const tipElement = document.getElementById('visitor-tip-text');
    const tipIcon = tipElement.parentElement.querySelector('i');
    tipElement.textContent = greeting;
    if (tipIcon) {
      tipIcon.className = icon;
    }
  }
  
  function handleError(message) {
    document.getElementById('visitor-ip-text').textContent = '获取失败';
    document.getElementById('visitor-location-text').textContent = '获取失败';
    document.getElementById('visitor-distance-text').textContent = '距离未知';
    document.getElementById('visitor-text').innerHTML = '欢迎来访！';
    console.error('获取访问者信息失败:', message);
  }
  
  // 计算两点间距离的函数（使用Haversine公式）
  function calculateDistance(lat1, lon1, lat2, lon2) {
    const R = 6371; // 地球半径，单位：千米
    const dLat = deg2rad(lat2 - lat1);
    const dLon = deg2rad(lon2 - lon1);
    const a = Math.sin(dLat/2) * Math.sin(dLat/2) +
              Math.cos(deg2rad(lat1)) * Math.cos(deg2rad(lat2)) *
              Math.sin(dLon/2) * Math.sin(dLon/2);
    const c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1-a));
    return R * c;
  }
  
  function deg2rad(deg) {
    return deg * (Math.PI/180);
  }
  
  // 启动获取位置信息
  tryGetLocation();
  
  // 每分钟更新一次问候语
  setInterval(updateGreeting, 60000);
})(); </script></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">welcome！<br/><img src="/imgs/gifs/1.gif" title="1.gif" class="announcementImg"/></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%9C%E4%B8%9A%E9%A2%98%E6%B1%87%E6%80%BB"><span class="toc-number">1.</span> <span class="toc-text">作业题汇总</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%9C%E4%B8%9A1"><span class="toc-number">1.1.</span> <span class="toc-text">作业1</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%9E%E7%BB%AD%E5%B1%9E%E6%80%A7%E4%B8%8E%E7%A6%BB%E6%95%A3%E5%B1%9E%E6%80%A7"><span class="toc-number">1.1.1.</span> <span class="toc-text">连续属性与离散属性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A6%BB%E7%BE%A4%E7%82%B9%EF%BC%88Outliers%EF%BC%89"><span class="toc-number">1.1.2.</span> <span class="toc-text">离群点（Outliers）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="toc-number">1.1.3.</span> <span class="toc-text">数据挖掘的定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9D%87%E5%80%BC%E4%B8%8E%E4%B8%AD%E5%80%BC"><span class="toc-number">1.1.4.</span> <span class="toc-text">均值与中值</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5%EF%BC%9A%E7%B2%BE%E5%BA%A6%E4%B8%8E%E5%8F%AC%E5%9B%9E%E7%8E%87"><span class="toc-number">1.1.5.</span> <span class="toc-text">混淆矩阵：精度与召回率</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%9C%E4%B8%9A2"><span class="toc-number">1.2.</span> <span class="toc-text">作业2</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A6%BB%E6%95%A3%E5%B1%9E%E6%80%A7%E7%BC%96%E7%A0%81"><span class="toc-number">1.2.1.</span> <span class="toc-text">离散属性编码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#PR%E6%9B%B2%E7%BA%BF%E4%B8%8EROC%E6%9B%B2%E7%BA%BF"><span class="toc-number">1.2.2.</span> <span class="toc-text">PR曲线与ROC曲线</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%9C%E4%B8%9A3"><span class="toc-number">1.3.</span> <span class="toc-text">作业3</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%AD%E7%9A%84%E5%B1%9E%E6%80%A7%E5%A4%84%E7%90%86"><span class="toc-number">1.3.1.</span> <span class="toc-text">线性回归中的属性处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8ERANSAC%E7%AE%97%E6%B3%95"><span class="toc-number">1.3.2.</span> <span class="toc-text">线性回归与RANSAC算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#RANSAC-RANdom-SAmple-Consensus-%E7%AE%97%E6%B3%95"><span class="toc-number">1.3.3.</span> <span class="toc-text">RANSAC (RANdom SAmple Consensus) 算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">1.3.4.</span> <span class="toc-text">正则化方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B2%AD%E5%9B%9E%E5%BD%92%EF%BC%88Ridge-Regression%EF%BC%89%E4%B8%8E-Lasso%E5%9B%9E%E5%BD%92%EF%BC%88Lasso-Regression%EF%BC%89"><span class="toc-number">1.3.5.</span> <span class="toc-text">岭回归（Ridge Regression）与 Lasso回归（Lasso Regression）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E5%AF%B9%E6%AF%94"><span class="toc-number">1.3.5.1.</span> <span class="toc-text">核心概念对比</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC%E8%A6%81%E7%82%B9"><span class="toc-number">1.3.5.2.</span> <span class="toc-text">数学推导要点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E5%8F%82%E6%95%B0%CE%BB%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-number">1.3.5.3.</span> <span class="toc-text">正则化参数λ的选择</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E5%AF%B9%E6%AF%94"><span class="toc-number">1.3.5.4.</span> <span class="toc-text">实际应用对比</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%BC%B9%E6%80%A7%E7%BD%91%E7%BB%9C%EF%BC%88Elastic-Net%EF%BC%89"><span class="toc-number">1.3.5.5.</span> <span class="toc-text">弹性网络（Elastic Net）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E5%8F%82%E6%95%B0%CE%BB%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">1.3.6.</span> <span class="toc-text">正则化参数λ的影响</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%9C%E4%B8%9A4-1"><span class="toc-number">1.4.</span> <span class="toc-text">作业4-1</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%92%E5%88%86%E5%87%86%E5%88%99"><span class="toc-number">1.4.1.</span> <span class="toc-text">决策树划分准则</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Gini%E6%8C%87%E6%95%B0%EF%BC%88Gini-Index%EF%BC%89"><span class="toc-number">1.4.2.</span> <span class="toc-text">Gini指数（Gini Index）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97Gini%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="toc-number">1.4.2.1.</span> <span class="toc-text">计算Gini的例子</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%86%B5%EF%BC%88Entropy%EF%BC%89"><span class="toc-number">1.4.3.</span> <span class="toc-text">熵（Entropy）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%86%B5%E4%B8%8EGini%E6%8C%87%E6%95%B0%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">1.4.3.1.</span> <span class="toc-text">熵与Gini指数的关系</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8D%E7%BA%AF%E5%BA%A6%E5%BA%A6%E9%87%8F%EF%BC%9A%E7%86%B5%E4%B8%8E%E5%88%86%E7%B1%BB%E8%AF%AF%E5%B7%AE"><span class="toc-number">1.4.4.</span> <span class="toc-text">不纯度度量：熵与分类误差</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E5%85%83%E5%88%92%E5%88%86"><span class="toc-number">1.4.5.</span> <span class="toc-text">二元划分</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%9C%E4%B8%9A4-2"><span class="toc-number">1.5.</span> <span class="toc-text">作业4-2</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Gini%E7%B3%BB%E6%95%B0%E8%AE%A1%E7%AE%97%E9%A2%98"><span class="toc-number">1.5.1.</span> <span class="toc-text">Gini系数计算题</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%9C%E4%B8%9A5"><span class="toc-number">1.6.</span> <span class="toc-text">作业5</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#K%E8%BF%91%E9%82%BB%E5%88%86%E7%B1%BB%E5%99%A8%EF%BC%88KNN%EF%BC%89"><span class="toc-number">1.6.1.</span> <span class="toc-text">K近邻分类器（KNN）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E6%B3%95"><span class="toc-number">1.6.2.</span> <span class="toc-text">贝叶斯分类法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%9C%E4%B8%9A6"><span class="toc-number">1.7.</span> <span class="toc-text">作业6</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Kmeans%E4%B8%8EDBSCAN%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AF%B9%E6%AF%94"><span class="toc-number">1.7.1.</span> <span class="toc-text">Kmeans与DBSCAN聚类算法对比</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Kmeans%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95"><span class="toc-number">1.7.2.</span> <span class="toc-text">Kmeans聚类算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DBSCAN%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95"><span class="toc-number">1.7.3.</span> <span class="toc-text">DBSCAN聚类算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Kmeans%E4%B8%8EDBSCAN%E5%AF%B9%E6%AF%94%E6%80%BB%E7%BB%93"><span class="toc-number">1.7.4.</span> <span class="toc-text">Kmeans与DBSCAN对比总结</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%82%BB%E8%BF%91%E5%BA%A6%E7%9F%A9%E9%98%B5"><span class="toc-number">1.7.5.</span> <span class="toc-text">邻近度矩阵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95"><span class="toc-number">1.7.6.</span> <span class="toc-text">层次聚类算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%B8%8E%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F"><span class="toc-number">1.7.7.</span> <span class="toc-text">相似度与距离度量</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2026/01/09/MachineCollectionFinalReview/" title="机器学习与数据挖掘期末复习"><img src="/imgs/ArticleTopImgs/MachineCollectionFinalReviewTopImg.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="机器学习与数据挖掘期末复习"/></a><div class="content"><a class="title" href="/2026/01/09/MachineCollectionFinalReview/" title="机器学习与数据挖掘期末复习">机器学习与数据挖掘期末复习</a><time datetime="2026-01-09T06:46:13.000Z" title="发表于 2026-01-09 14:46:13">2026-01-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/01/06/osFinalReview/" title="操作系统期末复习"><img src="/imgs/ArticleTopImgs/osFinalReviewTopImg.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="操作系统期末复习"/></a><div class="content"><a class="title" href="/2026/01/06/osFinalReview/" title="操作系统期末复习">操作系统期末复习</a><time datetime="2026-01-06T05:10:37.000Z" title="发表于 2026-01-06 13:10:37">2026-01-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/01/04/computerNetFinalReview/" title="计算机网络期末复习"><img src="/imgs/ArticleTopImgs/computerNetFinalReviewTopImg.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计算机网络期末复习"/></a><div class="content"><a class="title" href="/2026/01/04/computerNetFinalReview/" title="计算机网络期末复习">计算机网络期末复习</a><time datetime="2026-01-04T04:51:40.000Z" title="发表于 2026-01-04 12:51:40">2026-01-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/24/dataCollectionFinalReview/" title="数据采集实践期末复习"><img src="/imgs/ArticleTopImgs/dataCollectionFinalReviewTopImg.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据采集实践期末复习"/></a><div class="content"><a class="title" href="/2025/12/24/dataCollectionFinalReview/" title="数据采集实践期末复习">数据采集实践期末复习</a><time datetime="2025-12-24T07:22:31.000Z" title="发表于 2025-12-24 15:22:31">2025-12-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/11/28/EverydayAlgorithm/" title="每日算法"><img src="/imgs/ArticleTopImgs/EverydayAlgorithmTopImg.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="每日算法"/></a><div class="content"><a class="title" href="/2025/11/28/EverydayAlgorithm/" title="每日算法">每日算法</a><time datetime="2025-11-28T09:09:19.000Z" title="发表于 2025-11-28 17:09:19">2025-11-28</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/footerBg.png);"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2026 By XBXyftx</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><a target="_blank" rel="noopener" href="https://beian.miit.gov.cn/#/Integrated/index"><span>备案号：京ICP备2025126467号</span></a></div><div></div><span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now = new Date(); 
function createtime() { 
var grt = new Date("4/25/2024 18:30"); // 在此处修改你的建站时间
now.setTime(now.getTime() + 250); 
days = (now - grt) / 1000 / 60 / 60 / 24; 
dnum = Math.floor(days); 
hours = (now - grt) / 1000 / 60 / 60 - (24 * dnum); 
hnum = Math.floor(hours); 
if (String(hnum).length == 1) { hnum = "0" + hnum; } 
minutes = (now - grt) / 1000 / 60 - (24 * 60 * dnum) - (60 * hnum); 
mnum = Math.floor(minutes); 
if (String(mnum).length == 1) { mnum = "0" + mnum; } 
seconds = (now - grt) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
snum = Math.round(seconds); 
if (String(snum).length == 1) { snum = "0" + snum; } 
document.getElementById("timeDate").innerHTML = "本网站已运行 " + dnum + " 天 "; 
document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; 
} 
setInterval(createtime, 250);</script></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><a class="rightMenu-item" href="javascript:window.history.back();"><i class="fa-solid fa-arrow-left"></i></a><a class="rightMenu-item" href="javascript:window.location.reload();"><i class="fa-solid fa-arrow-rotate-right"></i></a><a class="rightMenu-item" href="javascript:window.history.forward();"><i class="fa-solid fa-arrow-right"></i></a><a class="rightMenu-item" id="menu-radompage" href="javascript:window.location.href = window.location.origin;"><i class="fa-solid fa-house"></i></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-text"><a class="rightMenu-item" href="javascript:rmf.copySelect();"><i class="fa-solid fa-copy"></i><span>复制</span></a></div><div class="rightMenu-group rightMenu-line"><a class="rightMenu-item" href="javascript:rmf.switchReadMode();"><i class="fa-solid fa-book"></i><span>阅读模式</span></a></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/instant.page/5.1.0/instantpage.min.js" type="module"></script><script src="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/node-snackbar/0.1.16/snackbar.min.js"></script><!-- 瀑布流JavaScript - 只在首页加载--><!-- 文章打字机效果 - 只在文章页面加载--><script src="/js/typewriter-effect.js"></script><!-- 网络监控器 - 帮助定位503问题--><script src="/js/network-monitor.js"></script><script src="/js/topimg-monitor.js"></script><!-- 入场弹窗功能--><script src="/js/entrance-popup-config.js"></script><script src="/js/entrance-popup.js"></script><!-- 基础懒加载功能--><script src="/js/lazy-loading.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://twikooxbx.netlify.app/.netlify/functions/twikoo',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const init = (el = document, path = location.pathname) => {
    twikoo.init({
      el: el.querySelector('#twikoo-wrap'),
      envId: 'https://twikooxbx.netlify.app/.netlify/functions/twikoo',
      region: '',
      onCommentLoaded: () => {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      },
      ...option,
      path: isShuoshuo ? path : (option && option.path) || path
    })

    

    isShuoshuo && (window.shuoshuoComment.destroyTwikoo = () => {
      if (el.children.length) {
        el.innerHTML = ''
        el.classList.add('no-comment')
      }
    })
  }

  const loadTwikoo = (el, path) => {
    if (typeof twikoo === 'object') setTimeout(() => init(el, path), 0)
    else btf.getScript('/js/twikoo.js').then(() => init(el, path))
  }

  if (isShuoshuo) {
    'Twikoo' === 'Twikoo'
      ? window.shuoshuoComment = { loadComment: loadTwikoo }
      : window.loadOtherComment = loadTwikoo
    return
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script></div><canvas id="universe"></canvas><script defer src="/js/universe-optimized.js"></script><script defer src="/js/jquery-3.6.0.min.js"></script><script defer src="/js/rightmenu.js"></script><script src="/js/happy-title.js" async></script><script defer src="/js/lazy-loading-optimized.js"></script><script defer src="/js/lightbox-enhanced.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementsByClassName('recent-posts')[0];
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2025/06/24/typewriter/" alt=""><img width="48" height="48" src="/imgs/ArticleTopImgs/TypeWriteTopImg.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-06-24</span><a class="blog-slider__title" href="2025/06/24/typewriter/" alt="">Butterfly主题实现简介打字机</a><div class="blog-slider__text">详细介绍如何在Hexo的Butterfly主题中添加炫酷的打字机效果，包含完整的实现代码和配置方法。</div><a class="blog-slider__button" href="2025/06/24/typewriter/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2025/04/30/ToTheApril2025/" alt=""><img width="48" height="48" src="/imgs/ArticleTopImgs/AprilTopImg.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-04-30</span><a class="blog-slider__title" href="2025/04/30/ToTheApril2025/" alt="">致2025那个繁忙的4月</a><div class="blog-slider__text">人们总是匆匆向前，殊不知时光易逝啊……（本文图量较大加载较慢持续更新中！！！）</div><a class="blog-slider__button" href="2025/04/30/ToTheApril2025/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2025/03/16/yiDuo/" alt=""><img width="48" height="48" src="/imgs/ArticleTopImgs/yiduoTopImg.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-03-16</span><a class="blog-slider__title" href="2025/03/16/yiDuo/" alt="">关于鸿蒙的一多能力</a><div class="blog-slider__text">一多能力的学习笔记</div><a class="blog-slider__button" href="2025/03/16/yiDuo/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2025/02/04/V2/" alt=""><img width="48" height="48" src="/imgs/ArticleTopImgs/V2TopImg.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-02-04</span><a class="blog-slider__title" href="2025/02/04/V2/" alt="">浅谈鸿蒙状态管理V1——&gt;V2</a><div class="blog-slider__text">用实际代码测试与我的开发经验，解析鸿蒙状态管理V2中难以理解的内容，帮助开发者们更好的理解和使用状态管理V2版本。</div><a class="blog-slider__button" href="2025/02/04/V2/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2025/04/20/RemoteCommunication/" alt=""><img width="48" height="48" src="/imgs/ArticleTopImgs/RCPTopImg.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-04-20</span><a class="blog-slider__title" href="2025/04/20/RemoteCommunication/" alt="">远场通信服务</a><div class="blog-slider__text">本文将介绍鸿蒙远场通信服务</div><a class="blog-slider__button" href="2025/04/20/RemoteCommunication/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --></body></html>